# Introduction to Statistics {#intro-stats}

## What is statistics? {#what-is-statistics}

### Early and modern definitions

The word *statistics* has its roots in the Latin word *status* which means the state, and in the
middle of the 18th century was intended to mean
*collection, processing and use of data by the state*.
With the rapid industrialisation of Europe in the first half of the 19th century, statistics
became established as a discipline. This led to the formation of the Royal Statistical Society,
the premier professional association of statisticians in the UK and also world-wide, in 1834.
During this 19th century growth period, statistics acquired a new meaning as the *interpretation of data or methods of extracting information from data for decision making*. Thus
statistics has its modern meaning as the methods for
*collection, analysis and interpretation of data*.
Indeed, the Oxford English Dictionary defines *statistics* as

> The practice or science of collecting and analysing numerical data in large quantities, especially for the purpose of inferring
proportions in a whole from those in a representative sample.

Note that the word ‘state’ has gone from its definition. Instead, statistical methods are now
essential for **everyone** wanting to answer questions using data.

For example, will it rain tomorrow? Does eating red meat make us live longer? Is smoking
harmful during pregnancy? Is the new shampoo better than the old? Will the UK economy get
better after Brexit? At a more personal level: What degree classification will I get at graduation?
How long will I live for? What prospects do I have in the career I have chosen? How do I invest
my money to maximise the return? Will the stock market crash tomorrow?


### Uncertainty: the main obstacle to decision making

The main obstacle to answering the types of questions above is *uncertainty*, which means **lack of
one-to-one correspondence between cause and effect**. For example, having a diet of (well-cooked) red meat for a period of time is not going to kill me immediately. The effect of smoking
during pregnancy is difficult to judge because of the presence of other factors, e.g. diet and lifestyle;
such effects will not be known for a long time, e.g. at least until the birth. Thus it seems:

> **Uncertainty is the only certainty!**



### Statistics tames uncertainty

It is clear that we may never be able to get to the bottom of every case to learn the full truth
and so will have to make a decision under uncertainty; thus mistakes cannot be avoided!
If mistakes cannot be avoided, it is better to know how often we make mistakes (which
provides knowledge of the amount of uncertainty) by following a particular rule of decision
making.
Such knowledge could be put to use in finding a rule of decision making which does not betray
us too often, or which minimises the frequency of wrong decisions, or which minimises the
loss due to wrong decisions.

Thus we have the equation 
\[\text{Uncertain knowledge} +
  \text{Knowledge of the extent of uncertainty in it} =
  \text{Usable knowledge.}\]

Researchers often make guesses about scientific quantities. For example, try to guess my age.
These predictions are meaningless without the associated uncertainties. Instead, appropriate
data collection and correct application of statistical methods may enable us to make statements
like: I am 90% certain that the correct age is between 29 and 39 years. Remember, 

> to guess is cheap, to guess incorrectly is expensive  
-- <cite>old Chinese proverb.</cite>


### Why should I study statistics as part of my degree?

Studying statistics will equip you with the basic skills in data analysis and doing science with
data.
A decent level of statistical knowledge is required no matter what branch of mathematics,
engineering, science and social science you will be studying.
Learning statistical theories gives you the opportunity to practice your deductive mathematical skills on real life problems. In this way, you will improve at mathematical methods while
studying statistical methods.

>All knowledge is, in final analysis, history.  
>All sciences are, in the abstract, mathematics.  
>All judgements are, in their rationale, statistics.  
>-- <cite>Prof. C. R. Rao </cite>

### Lies, Damn Lies and Statistics?

Sometimes people say, "you can prove anything in statistics!" and many such jokes. Such remarks
bear testimony to the fact that often statistics and statistical methods are mis-quoted without
proper verification and robust justification. This is even more important in recent years of the global
pandemic when every day we have been showered with a deluge of numbers. 

Statistics can be very much
mis-used and mis-interpreted, and as statisticians it is our duty to
correctly apply statistical techniques to develop scientifically robust and strong arguments.
As discussed before statistical methods are the only viable tool whenever there is uncertainty in
decision making. In scientific investigations, statistics is an inevitable instrument in search of truth
when uncertainty cannot be totally removed from decision making. Of course, a statistical method
may not yield the best predictions in a very particular situation, but a systematic and robust
application of statistical methods will eventually win over pure guesses. For example, statistical
methods prove that cigarette smoking is bad for human health.


### What's in this module?

- **Chapter 1**: We will start with the basic statistics used in everyday life, e.g. mean, median,
mode, standard deviation, etc. Statistical analysis and report writing will be discussed. We
will also learn how to explore data using graphical methods. We will introduce the R
programming language to help us with these tasks. 
- **Chapter 2: Introduction to Probability**. We will define and interpret probability as a
measure of uncertainty. We will learn the rules of probability and then explore fun examples
of probability, e.g. the probability of winning the National Lottery.
- **Chapter 3: Random variables**. We will learn that the results of different random experiments lead to different random variables following distributions such as the binomial, and
normal. etc. We will learn their basic properties, e.g. mean and variance.
- **Chapter 4: Statistical Inference**. We will discuss basic ideas of statistical inference,
including techniques of point and interval estimation and hypothesis testing.



## Example data sets

 In this module, we will assume that we have data from $n$ randomly selected sampling units, 
 which we will conveniently denote by  $x_1, x_2, \dots, x_n$. 
 We will assume that these values are numeric, either discrete like
counts, e.g. number of road accidents, or continuous, e.g. heights of 4-year-olds, marks obtained
in an examination. 


We will use the following examples to demonstrate ideas throughout the module:

::: {.example name="Fast food service time" #fastfood}
This dataset contains the service times (in seconds) of customers at a fast-food
restaurant. The first row is for customers who were served from 9–10am and the second row is for
customers who who were served from 2–3pm on the same day.


```{r, echo = FALSE}
fastfood <- tibble(time = rep(c("AM", "PM"), times = 10),
                   service = c(38, 45, 100, 62, 64, 52, 43, 72, 63, 81, 59, 88,
                               107, 64, 52, 75, 86, 59, 77, 70))
                   
fastfood %>%
    mutate(customer = rep(1:10, each = 2)) %>%
    pivot_wider(names_from = "time", values_from = "service") %>%
    select(-customer) %>%
    t %>%
    kable
```
We would like to compare these AM and PM service times.
:::

::: {.example name="Computer failures" #compfail}
This dataset contains 
weekly failures of a university computer system over a period of two years.

```{r, echo = FALSE, comment = ""}
compfail <- scan("compfail.txt")
cat(compfail, fill = TRUE)
```
We would like to summarise this data and make predictions about future failures.
:::

::: {.example name="Weight gain" #wtgain}
Is it true that students tend to gain weight during their first year in college? Cornell Professor
of Nutrition, David Levitsky, recruited students from two large sections of an introductory health
course. Although they were volunteers, they appeared to match the rest of the class in
terms of demographic variables such as sex and ethnicity. 68 students were weighed during the first
week of the semester, then again 12 weeks later. The first 10 rows of the data are:

```{r, echo = FALSE, message = FALSE}
wtgain <- read_table("wtgain.txt")
kable(head(wtgain, n = 10), col.names = c("Student number", "Initial weight (kg)", "Final weight (kg)"))
```
We would like to explore the data graphically and to
test whether the data support the hypothesis that students gain 
weight during their first year in college.
:::

::: {.example name="Billionaires" #billionaires}
Fortune magazine publishes a list of the world's billionaires each year. The 1992 list includes
225 individuals. Their wealth, age, and geographic location (Asia, Europe, Middle East, United
States, and Other) are reported. 

The variables in the data are:

- `wealth`: Wealth of family or individual in billions of dollars
- `age`: Age in years (for families it is the maximum age of family members)
- `region`: Region of the World (Asia, Europe, Middle East, United States and Other). 
The first 10 rows of the data are:

```{r, echo = FALSE, message = FALSE}
billionaires <- read_table("billionaires.txt") %>%
    mutate(region = recode(region, "M" = "Middle East",
                           "U" = "United States",
                           "A" = "Asia",
                           "E" = "Europe",
                           "O" = "Other"))
kable(head(billionaires, n = 10))
```
We will investigate differences in wealth of billionaires by age and
region using many exploratory graphical tools and statistical methods.
:::

## Introduction to R

R is a programming language for statistics. We will use R as a calculator,
to summarise data, make exploratory plots,
perform statistical analysis, illustrate
theorems and calculate probabilities. 
You will get practice of using R yourself during the computer practical sessions.

R is freely available to download:  search
"download R" or go to: https://cran.r-project.org/. 
We will access R via the RStudio integrated development environment,
which you can download from https://www.rstudio.com/products/rstudio/.
which provides a nice editor for R code alongside a console.
Details about using R and RStudio are given in the worksheets for 
the practical sessions.
If you want to complete the worksheets on your own computer,
please try to install R and RStudio before attending the practical sessions.

We will not give details of using R in these notes: this will
all be covered by the worksheets. In the
notes, we provide simple commands used to calculate useful
quantities in R, and show some of the graphical outputs
we can get from R.

## Summarising data sets {#summaries}

### Summarising categorical data

We can
summarise categorical (not numeric) data by tables. For example,
we could summarise the results of 20 coin tosses as: 12 heads, 8 tails.

For the computer failure data (Example \@ref(exm:compfail)) we may
summarise the count of failures per week:
```{r, echo = FALSE}
kable(t(table(compfail)))
```

### Measures of location

#### Choosing a representative value for the data

We are seeking a representative value for the data 
$x_1, x_2, \dots, x_n$ which should be a function
of the data. If $a$ is that representative value then how much error is associated with it?
The total error could be the sum of squares of the errors, 
\[\text{SSE}(a) = \sum_{i=1}^n (x_i - a)^2\]
or the sum of the absolute errors
\[\text{SAE}(a) = \sum_{i=1}^n |x_i - a|.\]
What value of $a$ will minimise the SSE or the SAE? For SSE the answer is the sample mean
and for SAE the answer is the sample median.


#### The sample mean

The *sample mean* is
\[\bar x = \frac{1}{n} (x_1 + x_2 + \dots + x_n)
  = \frac{1}{n} \sum_{i=1}^n x_i.\]
  For the service time
data from Example \@ref(exm:fastfood), 
the AM mean time is 68.9 seconds and the PM mean time is 66.8 seconds.
  
::: {.theorem}
The sample mean $\bar x$ minimises the SSE.
:::


::: {.proof}
We have
\begin{align*}
\text{SSE}(a) &= \sum_{i=1}^{n}\left(x_{i}-a\right)^{2} \\
&=\sum_{i=1}^{n}\left(x_{i}-\bar{x}+\bar{x}-a\right)^{2} \quad \text{(Add and subtract $\bar{x}$)} \\
&=\sum_{i=1}^{n}\left\{\left(x_{i}-\bar{x}\right)^{2}+2\left(x_{i}-\bar{x}\right)(\bar{x}-a)+(\bar{x}-a)^{2}\right\} \\
&=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+2(\bar{x}-a) \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)+\sum_{i=1}^{n}(\bar{x}-a)^{2} \\
&=\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}+n(\bar{x}-a)^{2},
\end{align*}
since $\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)=n \bar{x}-n \bar{x}=0$.

The first term is free of $a$ and the second term is non-negative for any value
of $a$. Hence the minimum occurs when the second term is zero, i.e. when $a = \bar x$.
:::

We have established the fact that
the sum of (or mean) squares of the deviations from any number $a$ is minimised when $a$ is the mean.
This justifies why we often use the mean as a representative value. 

#### The sample median

The *sample median* is the middle value in the ordered list of observations
$x_{(1)} \leq x_{(2)} \leq \dots \leq x_{(n)}$.
For the AM service time data \[38 < 43 < 52 < 59 < 63 < 64 < 77 < 86 < 100 < 107.\]
If $n$ is odd, there is a unique middle value.
If $n$ is even, there are two middle values (63 and 64 for the AM service time data).
Any value betwen those two middle values is a sample median. 
By convention, we often use the mean of these values
(63.5 for the the AM service time data).

::: {.theorem}
The sample median minimises the SSE.
:::

::: {.proof}
If $a<x_{(1)}$, then
\begin{equation}
\text{SAE}(a) = \sum_{i=1}^n (x_i - a).
(\#eq:SAE-lower)
\end{equation}
As $a$ increases, each term of \@ref(eq:SAE-lower) 
decreases until $a$ reaches $x_{(1)}$, so $\text{SAE} (x_{(1)})< \text{SAE}(a)$ for all $a<x_{(1)}$.
We conclude the minimise of the SAE is at least $x_{(1)}$.

Now suppose $x_{(k)} \leq a < x_{(k+1)}$ and let $\epsilon > 0$ be a small
number such that 
 $a + \epsilon < x_{(k+1)}$. Then
\begin{align*}
\text{SAE}(a + \epsilon) &=\sum_{i=1}^k\left(a + \epsilon - x_{(i)}\right)+\sum_{i=k+1}^n\left(x_{(i)}-(a+\epsilon)\right) \\
&= k \epsilon +\sum_{i=1}^k\left(a-x_{(i)}\right)-(n-k)\epsilon+\sum_{i=k+1}^n\left(x_{(i)}-a\right) \\
&=(2 k-n) \epsilon+\sum_{i=1}^k\left(a-x_{(i)}\right)+\sum_{i=k+1}^n\left(x_{(i)}-a\right) \\
&= (2 k-n)\epsilon + \text{SAE}(a)
\end{align*}
so $\text{SAE}(a+\epsilon)-\text{SAE}(a)=(2 k-n)\epsilon$. This is negative if $k < \frac{n}{2}$, zero if $k = \frac{n}{2}$, and positive if $k>\frac{n}{2}$. 

For each $k < \frac{n}{2}$, 
the SAE is decreasing
in the interval $[x_{(k)}, x_{(k+1)})$, and we conclude the minimiser of the SAE is at least $x_{(k+1)}$.
Starting at $k = 1$, we continue increasing $k$ by one and concluding the the minimiser of the SAE is at least $x_{(k+1)}$,
until we reach a $k$ such that $k \geq \frac{n}{2}$:

- If $n$ is odd, this happens at
$k = \frac{n+1}{2}$. In that case, since $k > \frac{n}{2}$,
the SAE is increasing in the interval $[x_{(k)}, x_{(k+1)})$, so we conclude the SAE is minimised
at $x_{(k)} = x_{\left(\frac{n+1}{2}\right)}$, the median point. 
- If $n$ is even, this happens at $k = \frac{n}{2}$. In that case the SAE is constant
in the interval $[x_{(k)}, x_{(k+1)})$, so we conclude that the SAE is minimised at any $a$ between
the two middle points
$x_{\left(\frac{n}{2}\right)}$ and $x_{\left(\frac{n}{2}+1\right)}$, i.e. any median point.
:::


We have
established the fact that
the sum of (or mean) of the absolute deviations from any number $a$ is
minimised when $a$ is the median. This justifies why
median is also often used as a representative value.

The mean gets more affected by extreme observations than the median. For example for
the AM service times, suppose the next observation is 190. The median will be 64 instead of 63.5
but the mean will shoot up to 79.9.

#### The sample mode

The mode or the most frequent (or the most likely) value in the data is taken as the most representative value if we consider a 0-1 error function instead of the SAE or SSE above. Here, one assumes
that the error is 0 if our guess $a$ is the correct answer and 1 if it is not. It can then be proved that the best guess $a$ will be the mode of the data.


### Measures of spread

A quick measure of the spread is the *range*, which is defined as the difference between the
maximum and minimum observations. For the AM service times the range is $69$ ($107 - 38$)
seconds.

The *variance* is
\[\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}.\]
We have
\[\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}=\sum_{i=1}^{n}\left(x_{i}^{2}-2 x_{i} \bar{x}+\bar{x}^{2}\right)=\sum_{i=1}^{n} x_{i}^{2}-2 \bar{x}(n \bar{x})+n \bar{x}^{2}=\sum_{i=1}^{n} x_{i}^{2}-n \bar{x}^{2},\]
so we calculate variance as
$$
\operatorname{Var}(x)=\frac{1}{n-1}\left(\sum_{i=1}^{n} x_{i}^{2}-n \bar{x}^{2}\right).
$$
Sometimes the variance is defined with the divisor $n$ instead of $n - 1$. We have chosen $n - 1$
since this is the default in R. We will return to this in Chapter \@ref(inference).

The *standard deviation* (sd) is the square root of variance
\[s = \operatorname{sd}(x) = \sqrt{\operatorname{Var}(x)}.\]
The standard deviation for the AM service times is 23.2 seconds. Note that it has the
same unit as the observations.

The *interquartile range* (IQR) is the difference between the third, $Q_3$ and first, $Q_1$ quartiles,
which are respectively the observations ranked $\frac{1}{4}(3n + 1)$ and $\frac{1}{4}(n + 3)$
in the ordered list.
Note that the median is the second quartile, $Q_2$. When $n$ is even, definitions of $Q_3$ and $Q_1$
are similar to that of the median, $Q_2$. The IQR for the AM service times is $83.75 - 53.75 = 30$
seconds.

### Summarising data in R

We can calculate the mean, median, variance and standard deviation
very easily in R. 

As an example, suppose we have stored the
weekly counts of computer failures from Example \@ref(exm:compfail)
in an object called `compfail` in R. You will
find out how to read the data into R in the practical worksheets.

Then we can calculate these quantities with:
```{r}
mean(compfail)
median(compfail)
var(compfail)
sd(compfail)
```
R saves us a lot of effort here relative to computing
these quantities by hand.

The `summary` command provides several using summary statistics:
```{r}
summary(compfail)
```

We can find a table summarising the counts by typing
```{r}
table(compfail)
```

## Exploratory data plots

### Introduction 
While the simple numerical summaries we have seen
in Section \@ref(summaries) provide useful insight
into data, often the clearest way to generate
insight is to plot the data. 
Plots may be used to check the shape of the distribution 
of a variable, and to investigate
the relationship between different variables
in a dataset.

The type of plot which is most appropriate often
depends on the type of data, and whether we are
interested in looking at a single variables or the 
relationship between two variables.

Here, we will show some plots produced by R. You will find
out how to make these plots in R in the computer worksheets.


### Distribution of a single discrete variable

To explore the distribution of a single discrete variable, 
we may use a bar plot. For instance, we may
plot the weekly counts of
computer failures from Example \@ref(exm:compfail):

```{r, echo = FALSE}
compfail_by_week <- tibble(week = 1:length(compfail),
                           failures = compfail)
ggplot(compfail_by_week) +
    geom_bar(aes(x = failures)) +
    xlab("Number of failures in a week")
```

This plot shows the same information as in the summary
table of counts, but it is easier to process the information
visually.


### Distribution of a single continuous variable

There are several different types of plots which may be 
used to summarise the shape of the
distribution of a single continuous variables.
For instance, suppose we are interested in the gain in weight
in Example \@ref(exm:wtgain).

We can use a box-and-whiskers 
plot to summarise the distribution of the weight difference:

```{r}
ggplot(wtgain_diff) +
    geom_boxplot(aes(x = diff)) +
    xlab("Final weight - Initial weight (kg)")
```

The line
in the middle of the box is the median service weight difference,
and the box runs from the first quartile
to the third quartile.
The lines (or whiskers) extend from the smallest value no further 
further than $1.5 \times \text{IQR}$ from the first quartile
to the largest value no
further than $1.5 \times \text{IQR}$ from the third quartile.

Any outliers, which are outisde the range of the whiskers,
are shown as separate points.

We can see the the majority of students in the sample do gain weight 
over the course of the study.

To get more information about the shape of the distribution,
we could plot 
a histogram of the weight difference:

```{r, echo = FALSE}
wtgain_diff <- wtgain %>%
    mutate(diff = final - initial)

ggplot(wtgain_diff) +
    geom_histogram(aes(x = diff), binwidth = 0.5, boundary = 0) +
    xlab("Final weight - Initial weight (kg)") +
    geom_vline(xintercept = 0, linetype = "dashed")
```
<!-- TODO: note impact of binwidth -->

Alternatively, we could draw a density plot
```{r, echo = FALSE}
ggplot(wtgain_diff) +
    geom_density(aes(x = diff)) +
    xlab("Final weight - Initial weight (kg)") +
    geom_vline(xintercept = 0, linetype = "dashed")
```

This plots an estimate of the probability density function,
which we will learn about in Chapter \@ref(distributions).

### Relationship between continuous and discrete variables

We can also use a box-and-whiskers plot to show how a continuous
variable changes with a discrete
one. For instance, for the service time data from Example \@ref(exm:fastfood),
we are interested in how AM and PM service times compare:

```{r, echo = FALSE}
ggplot(data = fastfood) +
    geom_boxplot(aes(service, time)) +
    xlab("Service time (minutes)") +
    ylab("Time of day")
```

Since no outlying points are shown
in this case, the whiskers show the range of the data.

We can also use any of the other methods for plotting 
a single continuous variable, and draw separate
plots for each group in the discrete variable.

We can use histograms:
```{r, echo = FALSE}
ggplot(data = fastfood) +
    geom_histogram(aes(x = service), binwidth = 10) +
    xlab("Service time (minutes)") +
    facet_wrap(~time)
```

Or density plots:
```{r, echo = FALSE}
ggplot(data = fastfood) +
    geom_density(aes(x = service)) +
    xlab("Service time (minutes)") +
    facet_wrap(~time)
```

With density plots, we could also plot the densities
for the different groups on a single plot:
```{r, echo = FALSE}
ggplot(data = fastfood) +
    geom_density(aes(x = service, colour = time)) +
    xlab("Service time (minutes)")
```

### Relationship between two continuous variables

A scatterplot is a good way to look at the relationship
between two continuous variables.
For instance, for the weight data from Example \@ref(exm:wtgain),
we can plot the final weights against the initial weights:

```{r, echo = FALSE}
plot_wt <- ggplot(data = wtgain) +
    geom_point(mapping = aes(x = initial, y = final)) +
    xlab("Initial weight (kg)") +
    ylab("Final weight (kg)")

plot_wt
```

We can add the straight line $y = x$ to the plot:

```{r, echo = FALSE}
plot_wt_2 <- plot_wt +
    geom_abline(slope = 1, intercept = 0, linetype = "dashed")

plot_wt_2
```

We can see that the majority of the points lie above this $y = x$
line, so the majority of students in the sample gain weight 
over the course of the study, as we have seen before.
This appears to be the case irrespective
of the initial weight of the student.

