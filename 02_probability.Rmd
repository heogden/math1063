# Introduction to Probability {#intro-prob}

## Definitions of probability


Probabilities are often used to express the uncertainty of events of interest happening. For example,
we may say that: (i) I think 
it is highly likely that Manchester City will retain the premiership title this season
or to be more specific, I think there is more than an 80% chance that Manchester City
will keep the title;
(ii) the probability of a tossed fair coin landing heads is 0.5. 
As we have seen in Chapter \@ref(intro-stats), there is uncertainty
everywhere, and probability is the language we use to quantify this uncertainty.

The two examples above, football and tossing a coin, convey two different interpretations of
probability. The football probability is the commentator's own subjective belief. The
commentator certainly has not performed a large experiment involving all the 20 teams over the
whole (future) season under all playing conditions, players, managers and transfers. This notion
is known as subjective probability. Subjective probability gives a measure of the plausibility of
the proposition, to the person making it, in the light of past experience (e.g. Manchester City are the
current champions) and other evidence (e.g. they spent the maximum amount of money buying
players). There are plenty of other examples, e.g. I think there is a 70% chance that the FTSE
100 will rise tomorrow, or according to the Met Office there is a 40% chance that we will have a
white Christmas this year in Southampton. 

The second definition of probability comes from the long-term relative frequency of a result of
a random experiment (e.g. coin tossing) which can be repeated an infinite number of times under
essentially similar conditions.

Imagine we are able to repeat a random experiment
under identical conditions and count how many of those repetitions result in the event $A$. The
relative frequency of $A$, i.e. the ratio
\[\frac{\text{the number of repetitions resulting in $A$}}{\text{total number of repetitions}},\]
approaches a fixed limit value as the number of repetitions increases. This limit value is defined as
$P\{A\}$.

As a simple example, in the experiment of tossing a particular coin, suppose we are interested
in the event $A$ of getting a 'head'. We can toss the coin 1000 times (i.e. do 1000 replications of
the experiment) and record the number of heads out of the 1000 replications. Then the relative
frequency of $A$ out of the 1000 replications is the proportion of heads observed.

Sometimes, however, it is much easier to find $P\{A\}$ by using some 'common knowledge' about
probability. For example, if the coin in the example above is fair 
(i.e. $P\{\text{head}\} = P\{\text{tail}\})$, then
this information and the common knowledge that $P\{\text{head}\}+P\{\text{tail}\} = 1$
immediately imply that
$P\{\text{head}\} = 0.5$ and $P\{\text{tail}\} = 0.5$. 
The 'common knowledge' about probability
will be formalised as the axioms of probability, which form the foundation of probability theory.

## Some definitions 

Before we can state and use the axioms of probability, 
we need introduce some terminology. 

A *random experiment* is one in which we do not know exactly
what outcome the experiment will give, even though we can write down all the possible outcomes.
The set of all possible outcomes is 
called the *sample space* ($S$). For example, in a coin tossing experiment, $S
= \{\text{head}, \text{tail}\}$. If we toss two coins together, 
$S = \{\text{HH}, \text{HT}, \text{TH}, \text{TT}\}$ where $\text{H}$ and $\text{T}$ denote
respectively the outcome head and tail from the toss of a single coin.

An *event* is a particular result of the random experiment. For example, 
$\text{HH}$
(two heads) is an event when we toss two coins together. Similarly, at least one head e.g. 
$\{\text{HH}, \text{HT}, \text{TH}\}$ is an event as well. 
Events are denoted by capital letters $A, B, C, \ldots$ or $A_1, B_1, A_2$ etc., and
a single outcome is called an *elementary event*, e.g. $\text{HH}$. 
An event which is a group of elementary
events is called a *composite event*, e.g. at least one head. How to determine the probability of a
given event $A$, $P\{A\}$, is the focus of probability theory.

::: {.example name="Die throw" #die-throw}
Roll a six-faced die and observe the score on the uppermost face.
Here $S = \{1, 2, 3, 4, 5, 6\}$, which is composed of six elementary events.
:::

The *union* of two given events $A$ and B, denoted as ($A$ or $B$) or $A \cup B$, 
consists of the outcomes
that are either in A or B or both. 'Event $A \cup B$ occurs' means 
'either $A$ or $B$ occurs or both occur'.

For example, in Example \@ref(exm:die-throw), 
suppose $A$ is the event that *an even number is observed*. This
event consists of the set of outcomes 2, 4 and 6, i.e. $A = \{\text{an even number}\} = \{2, 4, 6\}$.
Suppose $B$ is the event that *a number larger than 3 is observed*. This event consists of the 
outcomes 4, 5 and 6, i.e. $B = \{\text{a number larger than 3}\} = \{4, 5, 6\}$. 
Hence the event $A \cup B =
\{\text{an even number or a number larger than 3}\} = \{2, 4, 5, 6\}$. 
Clearly, when a $6$ is observed, both $A$
and $B$ have occurred.

The *intersection* of two given events $A$ and $B$, denoted as ($A$ and $B$) or $A \cap B$, 
consists of the
outcomes that are common to both $A$ and $B$. 'Event $A \cap B$ occurs' means 'both $A$ and $B$ occur'. For
example, in Example \@ref(exm:die-throw), 
$A \cap B = \{4, 6\}$. Additionally, if $C = \{\text{a number less than 6}\} = \{1, 2, 3, 4, 5\}$,
the intersection of events $A$ and $C$ is the event $A \cap C = \{\text{an even number less than 6}\} = \{2, 4\}$.

The union and intersection of two events can be generalized in an obvious way to the union and
intersection of more than two events.

Two events $A$ and $D$ are said to be *mutually exclusive* if $A \cap D = \emptyset$, where $\emptyset$ denotes the empty
set, i.e. $A$ and $D$ have no outcomes in common. Intuitively, '$A$ and $D$ are mutually exclusive'
means '$A$ and $D$ cannot occur simultaneously in the experiment'.

In Example \@ref(exm:die-throw), 
if $D = \{\text{an odd number}\} = \{1, 3, 5\}$, then $A \cap D = \emptyset$ and so $A$ and $D$ are
mutually exclusive. As expected, $A$ and $D$ cannot occur simultaneously in the experiment.

For a given event $A$, the *complement* of $A$ is the event that consists of all the outcomes not in
$A$ and is denoted by $A^\prime$ . Note that $A \cup A^\prime = S$ and $A \cap A^\prime = \emptyset$.


## Axioms of probability

Here are the three axioms of probability:

**A1** $P\{S\}=1$.  
**A2** $0 \leq P\{A\} \leq 1$ for any event $A$.  
**A3** $P\{A \cup B\}=P\{A\}+P\{B\}$ provided that $A$ and $B$ are mutually exclusive events.

Here are some of the consequences of the axioms of probability:

(1) For any event $A, P\{A\}=1-P\left\{A^{\prime}\right\}$.
(2) From (1) and Axiom A1, $P\{\emptyset\}=1-P\{S\}=0$. Hence if $A$ and $B$ are mutually exclusive events, then $P\{A \cap B\}=0$.
(3) If $D$ is a subset of $E, D \subset E$, then $P\left\{E \cap D^{\prime}\right\}=P\{E\}-P\{D\}$ which implies for arbitrary events $A$ and $B, P\left\{A \cap B^{\prime}\right\}=P\{A\}-P\{A \cap B\}$.
(4) It can be shown by mathematical induction that Axiom A3 holds for more than two mutually exclusive events:
$$
P\left\{A_{1} \cup A_{2} \cup \cdots \cup A_{k}\right\}=P\left\{A_{1}\right\}+P\left\{A_{2}\right\}+\ldots+P\left\{A_{k}\right\}
$$
provided that $A_{1}, \ldots, A_{k}$ are mutually exclusive events.
Hence, the probability of an event A is the sum of the probabilities of the individual outcomes that make up the event.
(5) For the union of two arbitrary events, we have the General addition rule: For any two events $A$ and $B$
$$
P\{A \cup B\}=P\{A\}+P\{B\}-P\{A \cap B\} .
$$

:::{.proof}
We can write $A \cup B=\left(A \cap B^{\prime}\right) \cup(A \cap B) \cup\left(A^{\prime} \cap B\right)$. All three of these are mutually exclusive events. Hence,
$$
\begin{aligned}
P\{A \cup B\} &=P\left\{A \cap B^{\prime}\right\}+P\{A \cap B\}+P\left\{A^{\prime} \cap B\right\} \\
&=P\{A\}-P\{A \cap B\}+P\{A \cap B\}+P\{B\}-P\{A \cap B\} \\
&=P\{A\}+P\{B\}-P\{A \cap B\}.
\end{aligned}
$$

:::
(6) The sum of the probabilities of all the outcomes in the sample space $S$ is 1 .



## Using combinatorics to find probabilities

### Experiments with equally likely outcomes

For an experiment with $N$ equally likely possible outcomes, the axioms (and the consequences
above) can be used to find $P\{A\}$ of any event $A$ in the following way.

From consequence (4), we assign probability $1/N$ to each outcome.

For any event $A$, we find $P\{A\}$ by adding up $1/N$ for each of the outcomes in event $A$:
\[P \{A\} = \frac{\text{number of outcomes in $A$}}
{\text{total number of possible outcomes of the experiment}}.\]

For experiments with equally likely outcomes, the task of calculating the probability 
of an event therefore reduces to counting the number of outcomes in the event
and the total number of possible outcomes. 
In the following sections, will use ideas from combinatorics
(the mathematics of counting) to help us complete these tasks.

Return to Example \@ref(exm:die-throw)
where a six-faced die is rolled. Suppose that one wins a bet if a 6 is
rolled. Then the probability of winning the bet is $1/6$ as there are six possible outcomes in the
sample space and exactly one of those, 6, wins the bet. Suppose $A$ denotes the event that an
even-numbered face is rolled. Then $P\{A\} = 3/6 = 1/2$ as we can expect.

::: {.example name="Dice throw" #dice}
Roll 2 distinguishable dice and observe the scores. Here 
\[S =
\{(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), . . . , (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)\}\]
which consists of 36
possible outcomes or elementary events, $A_1, \ldots , A_{36}$.
What is the probability of the outcome $6$ in
both the dice? The required probability is $1/36$. What is the probability that the sum of the two
dice is greater than 6? How about the probability that the sum is less than any number, e.g. 8?

**Hint**: Write down the sum for each of the 36 outcomes and then find the probabilities asked just
by inspection. Remember, each of the 36 outcomes has equal probability $1/36$.
:::


### Multiplication rule of counting {#sec-multiplication-rule}

To complete a specific task, suppose one has to complete $k(\geq 1)$ sub-tasks sequentially.
If there are 
$n_i$
different ways to complete the $i$th sub-task ($i = 1, \ldots, k$) then there are 
$n_1 \times n_2 \times \dots \times n_k$ different
ways to complete the task.

::: {.example name="Bus routes"}
Suppose there are 7 routes to London from Southampton and
then there are 5 routes to Cambridge out of London. How many ways can
I travel to Cambridge from Southampton via London? The answer is
obviously 35. 
:::


### The number of permutations of $k$ from $n$: $P(n, k)$


Suppose we are asked to select $k(\geq 1)$ from the $n (n \geq k)$ available people and sit the 
$k$ selected people in $k$
(different) chairs. How many different ways are there to complete the task?

By considering the $i$th sub-task as selecting a person to sit in
the $i$th chair ($i = 1, \ldots, k$), it follows directly from the multiplication rule 
in Section \@ref(sec-multiplication-rule)
that there are $n(n-1) \ldots (n-[k -1])$
ways to complete the task. 

The number $n(n-1) \ldots (n-[k-1])$
is called the number of permutations
of $k$ from $n$ and denoted by
\[P(n, k) = n(n - 1) \ldots (n - [k - 1]).\]
In particular, when $k = n$ we have $P(n, n) = n(n - 1) \ldots 1$, which 
is called '$n$ factorial' and denoted
as $n!$. Note that $0!$ is defined to be $1$. We have
\[P(n, k)=n(n-1) \ldots(n-[k-1])=\frac{n(n-1) \ldots(n-[k-1]) \times(n-k) !}{(n-k) !}=\frac{n !}{(n-k) !}.\]

::: {.example name="Football"}
How many possible rankings are there for the 20 football teams in
the premier league at the end of a season? This number is given by $P(20, 20) = 20!$, which is a huge
number! How many possible permutations are there for the top 4 positions who will qualify to play
in Europe in the next season? This number is given by $P(20, 4) = 20 \times 19 \times 18 \times 17$.
:::

#### The number of combinations of $k$ from $n$: $\binom{n}{k}$


Suppose we are asked to select $k(\geq 1)$ from the $n$ ($n \geq k$) available people. 
Note that this task does NOT
involve sitting the $k$ selected people in $k$ (different) chairs.
We want to find the number of possible
ways to complete this task, which is denoted as $\binom{n}{k}$.

For this, let us reconsider the task of "selecting $k$ from the $n$ available people and
sitting the $k$ selected people in $k$ (different) chairs", which we already know from the discussion
above has $P(n, k)$ ways to complete.
Alternatively, to complete this task, one has to complete two sub-tasks sequentially. The first
sub-task is to select $k$ from the $n$ available people, which has $\binom{n}{k}$
ways. The second
sub-task is to sit the $k$ selected people in $k$ (different) chairs, which has $k!$ ways. 
It follows directly
from the multiplication rule that there are $\binom{n}{k} \times k!$ ways
to complete the task. Hence we have
\[ P(n, k)= \binom{n}{k} \times k!\]
so
\[\binom{n}{k} =\frac{P(n, k)}{k!}=\frac{n!}{(n-k)! k!}.\]

::: {.example name="Football"}
How many possible ways are there to choose 3 teams for the bottom
positions of the premier league table at the end of a season? This number is given by 
$\binom{20}{3} = 20 \times 19 \times 18/3!$, which does not take into consideration the rankings of the three bottom teams.
:::

:::{.example name="Microchips" #microchip}
A box contains 12 microchips of which 4 are faulty. A sample of size
3 is drawn from the box without replacement.

- How many selections of 3 can be made? $\binom{12}{3}$
- How many samples have all 3 chips faulty? $\binom{4}{3}$.
- How many selections have exactly 2 faulty chips? $\binom{8}{1} \binom{4}{2}$
- How many samples of 3 have 2 or more faulty chips? $\binom{8}{1} \binom{4}{2} + \binom{4}{3}$

:::


### Calculation of probabilities of events under sampling 'at random'

For the experiment of 'selecting a sample of size $n$ from a box of $N$ items without replacement',
a sample is said to be selected at random if all the possible samples of size n are equally likely to
be selected. All the possible samples are then equally likely outcomes of the experiment and so
assigned equal probabilities.

:::{.example name="Microchips continued"}
In Example \@ref(exm:microchip) assume that 3 microchips are selected at
random without replacement. Then

- each outcome (sample of size $3$) has probability $1/\binom{12}{3}$.
- $P\{\text{all 3 selected microchips are faulty}\} = \binom{4}{3}/ \binom{12}{3}$.
- $P\{\text{2 chips are faulty}\} = \binom{8}{1} \binom{4}{2}/ \binom{12}{3}$.
- $P\{\text{2 or more chips are faulty}\} = \big(\binom{8}{1} \binom{4}{2} + \binom{4}{3}\big)/\binom{12}{3}.$

:::

### A general 'urn problem'

Example \@ref(exm:microchip) is one particular case of the following general 
urn problem which can be solved by the
same technique.
A sample of size $n$ is drawn at random without replacement from a box of $N$ items containing
a proportion $p$ of defective items.

- How many defective items are in the box? $N p$. How many good items are there? $N (1 - p)$.
Assume these to be integers.
- The probability of exactly $x$ defective items in the sample of $n$ items is
\[\frac{\binom{Np}{x} \binom{N(1-p)}{n-x}}{\binom{N}{n}}.\]
- Which values of $x$ (in terms of $N$, $n$ and $p$) make this expression well defined?
We'll see later that these values of $x$ and the corresponding probabilities make up what is called
the *hyper-geometric* distribution.

:::{.example name="Selecting a committee" #committee}
There are 10 students available for a committee of
which 4 are men and 6 are women. A random sample of 3 students are chosen to form the committee
--- what is the probability that exactly one is a man?
The total number of possible outcomes of the experiment is equal to the number of ways of
selecting 3 students from 10 and is given by $\binom{10}{3}$. 
The number of outcomes in the event 'exactly one
is a man' is equal to the number of ways of selecting 3 students from 10 with exactly one man, and
given by $\binom{4}{1} \binom{6}{2}$
Hence
\begin{align*}
P \{\text{exactly one man}\} &= \frac{\text{number of ways of selecting one man and two women}}
 {\text{number of ways of selecting 3 students}} \\
&= \frac{\binom{4}{1} \binom{6}{2}}{\binom{10}{3}} = \frac{4 \times 15}{120} = \frac{1}{2}
\end{align*}

:::

:::{.example name="The National Lottery"}
In Lotto, a winning ticket has six numbers from 1 to 59
matching those on the balls drawn on a Wednesday or Saturday evening. The 'experiment' consists
of drawing the balls from a box containing 59 balls. The 'randomness', the equal chance of any set
of six numbers being drawn, is ensured by the spinning machine, which rotates the balls during the
selection process. What is the probability of winning the jackpot?
The total number of possible selections of six balls/numbers is $\binom{59}{6}$
There is only 1 selection for winning the jackpot. Hence
\[P\{\text{jackpot}\} = \frac{1}{\binom{59}{6}} = 2.22 \times 10^{-8},\]
which is roughly 1 in 45 million.

There is one other way of win a very large prize, of Â£1 million,
by using the bonus ball --- matching 5 of the selected 6 balls
plus matching the bonus ball. The probability of this is given by
\[P \{\text{$5$ matches + bonus}\} = \frac{6}{\binom{59}{6}} = 1.33 \times 10^{-7} .\]

Other smaller prizes are given for fewer matches.
The corresponding probabilities are:
\begin{align*}
P \{\text{$5$ matches}\} &= \frac{\binom{6}{5} \binom{53}{1}}{\binom{59}{6}} = 7.06 \times 10^{-6}. \\
P \{\text{$4$ matches}\} &= \frac{\binom{6}{4} \binom{53}{2}}{\binom{59}{6}} = 0.000459.\\
P \{\text{$3$ matches}\} &=\frac{\binom{6}{3} \binom{53}{3}}{\binom{59}{6}} = 0.0104.
\end{align*}

:::

## Conditional probability and Bayes' Theorem


How can we use additional information, i.e. things that have already happened, in
the calculation of probabilities? For example, a person may have a certain disease, e.g. diabetes or
HIV/AIDS, whether or not they show any symptoms of it. Suppose a randomly selected person is
found to have the symptom. Given this additional information, what is the probability that they
have the disease? Note that having the symptom does not fully guarantee that the person has the
disease.

Applications of conditional probability occur naturally in actuarial science and medical studies, where conditional probabilities such as "what is the probability that a person will survive for
another 20 years given that they are still alive at the age of 40?" are calculated.
In many real problems, one has to determine the probability of an event A when one already
has some partial knowledge of the outcome of an experiment, i.e. another event B has already
occurred. For this, one needs to find the conditional probability.

:::{.example name="Die throw continued"}
Return to the rolling of a fair die (Example \@ref(exm:die-throw)). Let
\begin{align*}
A = \{\text{a number greater than $3$}\} = \{4, 5, 6\},
B = \{\text{an even number}\} = \{2, 4, 6\}.
\end{align*}
It is clear that $P \{B\} = 3/6 = 1/2$. This is the unconditional probability of the event $B$. It is
sometimes called the *prior* probability of $B$.

However, suppose that we are told that the event $A$ has already occurred. What is the 
probability of $B$ now given that $A$ has already happened?

The sample space of the experiment is $S = \{1, 2, 3, 4, 5, 6\}$, which contains $n = 6$ 
equally likely outcomes.

Given the partial knowledge that event $A$ has occurred, only the $n_A = 3$ outcomes in 
$A = \{4, 5, 6\}$ could have occurred. However, only some of the outcomes in $B$ among these 
$n_A$ outcomes in $A$ will make event $B$ occur; the number of such outcomes is given by the 
number of outcomes
$n_{A\cap B}$ in both $A$ and $B$, i.e., $A \cap B$, and equal to $2$. 
Hence the probability of $B$, given the partial
knowledge that event $A$ has occurred, is equal to
\[\frac{2}{3} = \frac{n_{A \cap B}}{n_A} = \frac{n_{A\cap B} / n}{n_A / n}
= \frac{P\{A \cap B\}}{P\{A\}}\]
Hence we say that $P \{B|A\} = 2/3$, which is often interpreted as 
the *posterior* probability of $B$ given
$A$. The additional knowledge that $A$ has already occurred has helped us to 
revise the prior probability of $1/2$ to $2/3$.

:::

This simple example leads to the following general definition of conditional probability.

### Definition of conditional probability

For events $A$ and $B$ with $P \{A\} > 0$, the conditional probability of event 
$B$, given that event $A$ has
occurred, is
\[P \{B|A\} = \frac{P \{A \cap B\}}{P \{A\}}.\]


:::{.example}
Of all individuals buying a mobile phone, 60\% include a 64 GB hard disk in their
purchase, 40\% include a 16 MP camera and 30\% include both. If a randomly selected purchase
includes a 16 MP camera, what is the probability that a 64GB hard disk is also included?
The conditional probability is given by
\[P \{\text{$64$ GB}|\text{$16$ MP}\} =
  \frac{P \{\text{$64$ GB} \cap \text{$16$ MP}\}}{P\{\text{$16$ MP}\}}
  = \frac{0.3}{0.4} = 0.75.\]

:::

### Multiplication rule of conditional probability

By rearranging the conditional probability definition, we obtain the multiplication rule of conditional probability:
\[P\{A \cap B\} = P\{A\} P\{B|A\}.\]
Clearly the roles of A and B could be interchanged:
\[P\{A \cap B\} = P\{B\} P\{A|B\}.\]
 Hence the multiplication rule of conditional probability for two events is
\[P\{A \cap B\} = P\{B\}P\{A|B\} = P\{A\}P\{B|A\}.\]

It is straightforward to show by mathematical induction the following multiplication rule of conditional probability for $k(\geq 2)$ events $A_1 , A_2 , \ldots , A_k$:
\[P \{A_1 \cap A_2 \cap \ldots \cap A_k\} = P \{A_1\}P \{A_2 |A_1\} P\{A_3 |A_1 \cap A_2\} \ldots P\{A_k |A_1 \cap A_2 \cap \ldots \cap A_{k-1}\}.\]

:::{.example name="Selecting a committee continued"}
Return to the committee selection example
(Example \@ref(exm:committee)), 
where there are 4 men (M) and 6 women (W). We want to select a 2-person committee.
Find:

(i) the probability that both are men,
(ii) the probability that one is a man and the other is a woman.

We have already dealt with this type of urn problem by using the combinatorial method. Here,
the multiplication rule is used instead.
Let $M_i$ be the event that the $i$th person is a man, and $W_i$ be the event that the $i$th
 person is a woman, $i = 1, 2$. Then
\[\text{Prob in (i)} = P \{M_1 \cap M_2 \} = P \{M_1\}P \{M_2 |M_1\} = \frac{4}{10} \times \frac{3}{9},\]
\begin{align*}
\text{Prob in (ii)} &= P \{M_1 \cap W_2 \}+P \{W_1 \cap M_2 \} \\
&= P \{M_1\}P \{W_2 |M_1 \}+P \{W_1\}P \{M_2 |W_1\} \\
&= \frac{4}{10} \times \frac{6}{9} + \frac{6}{10} \times \frac{4}{9}
\end{align*}

You can find the probability that 'both are women' in a similar way.

:::

### Total probability formula

:::{.example name="Phones"}
Suppose that in our world there are only three phone manufacturing
companies: A Pale, B Sung and C Windows, and their market shares are respectively 30, 40 and 30
percent. Suppose also that respectively 5, 8, and 10 percent of their phones become faulty within
one year. If I buy a phone randomly (ignoring the manufacturer), what is the probability that my
phone will develop a fault within one year? After finding the probability, suppose that my phone
developed a fault in the first year --- what is the probability that it was made by A Pale?

```{r, echo = FALSE}
library(tidyverse)
phones_data <- tibble(Company = c("A Pale", "B Sung", "C Windows"),
                      "Market share" = c("30%", "40%", "30%"),
                      "Percent defective" = c("5%", "8%", "10%"))
center_tab(phones_data)
```
:::

To answer this type of question, we derive two of the most useful results in probability theory:
the total probability formula and Bayes' theorem. First, let us derive the total probability
formula.

Let $B_1, B_2, \ldots , B_k$ be a set of mutually exclusive, i.e.
\[B_i \cap B_j = \emptyset, \text{for all $1 \leq i \not = j \leq k$.}\]
and exhaustive events, i.e.:
\[B_1 \cup B_2 \cup \ldots \cup B_k = S.\]
Now any event $A$ can be represented by
\[A = A \cap S = (A \cap B_1) \cup (A \cap B_2) \cup \ldots \cup (A \cap B_k)\]
where $(A \cap B_1), (A \cap B_2), \ldots , (A \cap B_k)$
are mutually exclusive events. Hence the Axiom A3 of probability gives
\begin{align*}
P \{A\} &= P \{A \cap B_1 \} + P \{A \cap B_2 \} + . . . + P \{A \cap B_k \} \\
&= P \{B_1 \}P \{A|B_1 \} + P \{B_2 \}P \{A|B_2 \} + . . . + P \{B_k \}P \{A|B_k \}.
\end{align*}
This last expression is called the *total probability formula* for $P \{A\}$.

:::{.example name="Phones continued"}
 We can now find the probability of the event, say $A$, that
a randomly selected phone develops a fault within one year. Let $B_1, B_2, B_3$ be the events that the
phone is manufactured respectively by companies A Pale, B Sung and C Windows. Then we have:
\begin{align*}
P \{A\} &= P \{B_1 \}P \{A|B_1 \} + P \{B_2 \}P \{A|B_2 \} + P \{B_3 \}P \{A|B_3 \} \\
&= 0.30 \times 0.05 + 0.40 \times 0.08 + 0.30 \times 0.10 \\
&= 0.077.
\end{align*}

Now suppose that my phone has developed a fault within one year. What is the probability that
it was manufactured by A Pale? To answer this we need to introduce Bayes' Theorem.

:::

### Bayes' theorem

::: {.theorem name="Bayes' Theorem"}
Let $A$ and $B$ be events. Then
\[P \{B |A\} = \frac{P \{B \}P \{A|B \}}{P \{A\}}.\]
:::

::: {.proof}
From the definition of conditional probability, we have
\[P \{B |A\} =
  \frac{P \{B \cap A\}}{P \{A\}} =
  \frac{P \{B \}P \{A|B \}}{P \{A\}}.\]
:::


The probability $P \{B |A\}$ is called the posterior probability of 
$B$ given $A$ and $P \{B \}$ is called the prior
probability.
Bayes' theorem is the rule that converts the prior probability into the 
posterior probability by using the additional information that some other event,
$A$ above, has already
occurred. 

:::{.example name="Phones continued"}
The probability that my faulty phone was manufactured
by A Pale is
\[P \{B_1 |A\} = \frac{P \{B_1 \}P \{A|B_1 \}}{P \{A\}} =
\frac{0.30 \times 0.05}{0.077}
= 0.1948.\]
Similarly, the probability that the faulty phone was manufactured by B Sung is $0.4156$, and the
probability that it was manufactured by C Windows is $1-0.1948-0.4156 = 0.3896$.
:::

As in this example, we usually need to use the total probability formula
to calculate the denominator $P(A)$ in Bayes' theorem.


## Independent events


### Introduction and definition of independence

We have just seen that the probability of an event may change if we have additional
information. However, in many situations the probabilities may not change. For example, the
results of two coin tosses should not depend on each other.
In this section we will learn about the probabilities of independent events. Much of statistical
theory relies on the concept of independence.

We have seen examples where prior knowledge that an event $A$ has occurred has changed
the probability that event $B$ occurs. There are many situations where this does not happen.
The events
are then said to be independent.
Intuitively, events $A$ and $B$ are independent if the occurrence of one event does not affect the
probability that the other event occurs.
This is equivalent to saying that
$P \{B|A\} = P \{B\}$, where $P \{A\} > 0$, and $P \{A|B\} = P \{A\}$, where $P \{B\} > 0$.

These give the following formal definition: 
$A$ and $B$ are *independent* events if $P \{A \cap B\} = P \{A\}P \{B\}.$

:::{.example name="Die throw"}
Throw a fair die. Let $A$ be the event that "the result is even" and $B$ be the event that "the
result is greater than 3". We want to show that $A$ and $B$ are not independent.

For this, we have $P \{A \cap B\} = P \{\text{either a $4$ or $6$ thrown}\} = 1/3$,
but $P \{A\} = 1/2$ and $P \{B\} = 1/2$, so that
$P \{A\}P \{B\} = 1/4 \not = 1/3 = P \{A \cap B\}$.
Therefore $A$ and $B$ are not independent events.
:::


Independence is often assumed on physical grounds, although sometimes incorrectly. There are
serious consequences for wrongly assuming independence, e.g. the financial crisis in 2008. However,
when the events are independent then the simpler product formula for joint probability is then used.

:::{.example name="Dice throw"}
Two fair dice when shaken together are assumed to behave independently. Hence
the probability of two sixes is $1/6 \times 1/6 = 1/36$.
:::

:::{.example name="Assessing risk in legal cases"}
There have been some disastrous
miscarriages of justice as a result of incorrect assumption of independence. Please read "Incorrect
use of independence --- Sally Clark Case" on Blackboard.
<!-- TODO: get/write this document -->
:::


:::{.theorem name="Independence of complementary events"}
If $A$ and $B$ are independent, so are $A^\prime$
and $B^\prime$.
:::

:::{.proof}
Given that
$P \{A \cap B\} = P \{A\}P \{B\}$, we need to show that
$P \{A^\prime \cap B^\prime \} = P \{A^\prime \}P \{B^\prime \}$.
We have
\begin{align*}
P \{A^\prime \cap B^\prime \} &= 1 - P \{A \cup B\} \\
&= 1 - [P \{A\} + P \{B\} - P \{A \cap B\}] \\
&= 1 - [P \{A\} + P \{B\} - P \{A\}P \{B\}] \\
&= [1 - P \{A\}] - P \{B\}[1 - P \{A\}] \\
& = [1 - P \{A\}][1 - P \{B\}] \\
&= P \{A^\prime \}P \{B^\prime \}
\end{align*}
:::



### Independence with three events

The ideas of conditional probability and independence can be extended to more than two events.

Three events $A$, $B$ and $C$ are defined to be independent if
\begin{equation}
P \{A \cap B\} = P \{A\}P \{B\}, \; P \{A \cap C\} = P \{A\}P \{C\}, \; P \{B \cap C\} = P \{B\}P \{C\},
(\#eq:pairwise-indep)
\end{equation}
\begin{equation}
P \{A \cap B \cap C\} = P \{A\}P \{B\}P \{C\}.
(\#eq:threewise-indep)
\end{equation}

Note that \@ref(eq:pairwise-indep) does NOT imply \@ref(eq:threewise-indep)
as shown by the next example. Hence, to show the
independence of $A$, $B$ and $C$, it is necessary to show that both 
\@ref(eq:pairwise-indep) and \@ref(eq:threewise-indep) hold.

:::{.example}
A box contains eight tickets, each labelled with a binary number. Two are
labelled with the binary number $111$, two are labelled with $100$, two with $010$ and two with $001$.
An experiment consists of drawing one ticket at random from the box.
Let $A$ be the event "the first digit is 1", $B$ the event "the second digit is 1" 
and $C$ be the event
"the third digit is 1". 
It is clear that $P \{A\} = P \{B\} = P \{C\} = 4/8 = 1/2$ and 
$P \{A \cap B\} =
P \{A \cap C\} = P \{B \cap C\} = 1/4$, 
so the events are pairwise independent, i.e. \@ref(eq:pairwise-indep) holds.
However
$P \{A \cap B \cap C\} = 2/8 \not = P \{A\}P \{B\}P \{C\} = 1/8$. 
So \@ref(eq:threewise-indep) does not hold and $A$, $B$ and $C$ are not
independent.
:::


