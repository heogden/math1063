# Simple Linear Regression {#lm}

## What is regression?

So far, we have looked at simple statistical models, with a single
variable of interest. We have estimated the distribution of that
variable based on random samples from that one variable. In many
real-world problems, we instead want to understand the relationship
between multiple variables.

Regression seeks to model how one "output" variable, called the
**response** variable, $Y$, depends on one or more "input" variables
$x$, which we will call **covariates**. 

The response $Y$ is sometimes called the dependent variable. The
covariates $x$ are sometimes called independent or explanatory
variables.

In MATH1063, we will study only **simple** regression models, which
have a single covariate $x$.

For instance, suppose we interested in how the birth weight of babies
depends on gestational age (in weeks). The response, $Y$, is birth
weight in $g$. The covariate, $x$ is gestational age (in
weeks). Suppose we have the following data (this data is fictional,
but based on [real data for male singleton births in Canada](
https://www.canada.ca/en/public-health/services/injury-prevention/health-surveillance-epidemiology-division/maternal-infant-health/birth-weight-gestational.html)).

```{r, echo = FALSE}

x_grid <- 22:43

#' give weights in kg, not g, so divide by 1000
mu_grid <- c(501, 598, 697, 800, 909, 1026, 1159, 1312, 1487, 1682, 1896, 2123, 2361, 2607, 2855, 3091, 3306, 3489, 3638, 3745, 3800, 3793) / 1000

sd_grid <- c(108, 122, 136, 149, 163, 178, 195, 214, 234, 256, 278, 301, 324, 346, 368, 388, 405, 420, 432, 441, 446, 448) / 1000

mu_fun <- splinefun(x_grid, mu_grid)
sd_fun <- splinefun(x_grid, sd_grid)

set.seed(1)
n <- 40
x <- round(rnorm(n, mean = 38, sd = 3), digits = 1)
y <- mu_fun(x) + rnorm(n, sd = sd_fun(x))

library(tidyverse)
bw <- tibble(ga = x,
             weight = y) 

mod <- lm(weight ~ ga, data = bw)

p <- ggplot(bw, aes(x = ga, y = weight)) +
    geom_point() +
    xlab("Gestational age (weeks)") +
    ylab("Birth weight (g)")

p
```

## Simple linear regression

In simple linear regression, we model the dependence of the response
on the covariate as a straight line plus an error term. That is
\[Y_i
= \beta_0 + \beta_1 x_i + \epsilon_i, \; i = 1, \ldots, n,\]
where $\beta_0$ is an **intercept** parameter, $\beta_1$ is a
**slope** parameter and $\epsilon_i$ is a random error term. We
usually model $\epsilon_i \sim N(0, \sigma^2)$, with variance
parameter $\sigma^2$. There are three unknown parameters, $\beta_0$,
$\beta_1$ and $\sigma^2$, which we want to estimate from the data.

We have \[E(Y_i) = \beta_0 + \beta_1 x_i.\] Any choice of $\beta_0$
and $\beta_1$ give a different straight line. We call $\beta_0$ and
$\beta_1$ the **regression parameters**.

Out of all possible choices of straight line (choices of the
regression parameters $\beta_0$ and $\beta_1$), which one should we
choose?

## Estimating the regression parameters

To estimate the unknown regression parameters, we attempt to make the
associated straight line as close as possible to the data. We measure
the distance between the line and the data with the sum of squares
criterion:

\[\text{SS}(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2.\]

To estimate $\beta_0$ and $\beta_1$, we find the values which
minimise this sum of squares criterion. These estimates are called
the **least squares estimates**.

To find the least squares estimates, we take partial derivatives of
$SS(\beta_0, \beta_1)$ with respect to $\beta_0$ and $\beta_1$, set
them to zero, and solve for the parameters.

First, differentiate with respect to $\beta_0$, to give
\[
\frac{\partial SS}{\partial \beta_0} = -2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i).
\]
Setting to zero:
\[
\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i) = 0,
\]
so
\[
n\beta_0 + \beta_1 \sum_{i=1}^n x_i = \sum_{i=1}^n y_i,
\]
which gives
\[
\beta_0 = \bar{y} - \beta_1 \bar{x}.
\]

Next, differentiate with respect to $\beta_1$, to give
\[
\frac{\partial SS}{\partial \beta_1} = -2 \sum_{i=1}^n x_i (y_i - \beta_0 - \beta_1 x_i).
\]
Setting to zero gives
\[
\sum_{i=1}^n x_i (y_i - \beta_0 - \beta_1 x_i) = 0,
\]
or
\[
\sum_{i=1}^n x_i y_i - \beta_0 \sum_{i=1}^n x_i - \beta_1 \sum_{i=1}^n x_i^2 = 0.
\]

Substituting $\beta_0 = \bar{y} - \beta_1 \bar{x}$ gives
\[
\sum_{i=1}^n x_i y_i - (\bar{y} - \beta_1 \bar{x}) \sum_{i=1}^n x_i - \beta_1 \sum_{i=1}^n x_i^2 = 0,
\]
so
\[
\sum_{i=1}^n x_i y_i - n\bar{y}\bar{x} = \beta_1 \left( \sum_{i=1}^n x_i^2 - n\bar{x}^2 \right).
\]
So
\[
\beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}.
\]

Thus, the least squares estimates are
\[
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}, \qquad
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}.
\]

```{r, echo = FALSE}
SS <- function(beta0, beta1) {
    sum((y - beta0 - beta1 * x)^2)
}
```
## Estimation in R

In practice, we do not need to apply these formulas by hand to find the
least squares estimates. Instead, we can use the `lm` function in `R`
to find the estimates. For instance, if the birth weight data is
stored in a data frame called `bw`, with columns `weight` for birthweight
(which is the response, $y$) and `ga` for gestational age (which is the covariate, $x$), 
then we can fit the simple linear regression model with:

```{r}
mod <- lm(weight ~ ga, data = bw)
```

We can inspect the fitted model:

```{r}
mod
```

We see $\hat \beta_0 =$ `r round(coef(mod)[1], 4)` 
and $\hat \beta_1 =$ `r round(coef(mod)[2], 4)`.
