# Simple Linear Regression {#lm}

## What is regression?

So far, we have looked at simple statistical models, with a single
variable of interest. We have estimated the distribution of that
variable based on random samples from that one variable. In many
real-world problems, we instead want to understand the relationship
between multiple variables.

Regression seeks to model how one "output" variable, called the
**response** variable, $Y$, depends on one or more "input" variables
$x$, which we will call **covariates**. 

The response $Y$ is sometimes called the dependent variable. The
covariates $x$ are sometimes called independent or explanatory
variables.

In MATH1063, we will study only **simple** regression models, which
have a single covariate $x$.

For instance, suppose we interested in how the birth weight of babies
depends on gestational age (in weeks). The response, $Y$, is birth
weight in $g$. The covariate, $x$ is gestational age (in
weeks). Suppose we have the following data (this data is fictional,
but based on [real data for male singleton births in Canada](
https://www.canada.ca/en/public-health/services/injury-prevention/health-surveillance-epidemiology-division/maternal-infant-health/birth-weight-gestational.html)).

```{r, echo = FALSE}

x_grid <- 22:43

#' give weights in kg, not g, so divide by 1000
mu_grid <- c(501, 598, 697, 800, 909, 1026, 1159, 1312, 1487, 1682, 1896, 2123, 2361, 2607, 2855, 3091, 3306, 3489, 3638, 3745, 3800, 3793) / 1000

sd_grid <- c(108, 122, 136, 149, 163, 178, 195, 214, 234, 256, 278, 301, 324, 346, 368, 388, 405, 420, 432, 441, 446, 448) / 1000

mu_fun <- splinefun(x_grid, mu_grid)
sd_fun <- splinefun(x_grid, sd_grid)

set.seed(1)
n <- 40
x <- round(rnorm(n, mean = 38, sd = 3), digits = 1)
y <- mu_fun(x) + rnorm(n, sd = sd_fun(x))

library(tidyverse)
bw <- tibble(ga = x,
             weight = y) 

mod <- lm(weight ~ ga, data = bw)

p <- ggplot(bw, aes(x = ga, y = weight)) +
    geom_point() +
    xlab("Gestational age (weeks)") +
    ylab("Birth weight (g)")

p
```

## Simple linear regression

In simple linear regression, we model the dependence of the response
on the covariate as a straight line plus an error term. That is
\[Y_i
= \beta_0 + \beta_1 x_i + \epsilon_i, \; i = 1, \ldots, n,\]
where $\beta_0$ is an **intercept** parameter, $\beta_1$ is a
**slope** parameter and $\epsilon_i$ is a random error term. We
usually model $\epsilon_i \sim N(0, \sigma^2)$, with variance
parameter $\sigma^2$. There are three unknown parameters, $\beta_0$,
$\beta_1$ and $\sigma^2$, which we want to estimate from the data.

We have \[E(Y_i) = \beta_0 + \beta_1 x_i.\] Any choice of $\beta_0$
and $\beta_1$ give a different straight line. We call $\beta_0$ and
$\beta_1$ the **regression parameters**.

Out of all possible choices of straight line (choices of the
regression parameters $\beta_0$ and $\beta_1$), which one should we
choose?

## Estimating the regression parameters

To estimate the unknown regression parameters, we attempt to make the
associated straight line as close as possible to the data. We measure
the distance between the line and the data with the sum of squares
criterion:

\[\text{SS}(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2.\]

To estimate $\beta_0$ and $\beta_1$, we find the values which
minimise this sum of squares criterion. These estimates are called
the **least squares estimates**.

To find the least squares estimates, we take partial derivatives of
$SS(\beta_0, \beta_1)$ with respect to $\beta_0$ and $\beta_1$, set
them to zero, and solve for the parameters.

First, differentiate with respect to $\beta_0$, to give
\[
\frac{\partial SS}{\partial \beta_0} = -2 \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i).
\]
Setting to zero:
\[
\sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i) = 0,
\]
so
\[
n\beta_0 + \beta_1 \sum_{i=1}^n x_i = \sum_{i=1}^n y_i,
\]
which gives
\[
\beta_0 = \bar{y} - \beta_1 \bar{x}.
\]

Next, differentiate with respect to $\beta_1$, to give
\[
\frac{\partial SS}{\partial \beta_1} = -2 \sum_{i=1}^n x_i (y_i - \beta_0 - \beta_1 x_i).
\]
Setting to zero gives
\[
\sum_{i=1}^n x_i (y_i - \beta_0 - \beta_1 x_i) = 0,
\]
or
\[
\sum_{i=1}^n x_i y_i - \beta_0 \sum_{i=1}^n x_i - \beta_1 \sum_{i=1}^n x_i^2 = 0.
\]

Substituting $\beta_0 = \bar{y} - \beta_1 \bar{x}$ gives
\[
\sum_{i=1}^n x_i y_i - (\bar{y} - \beta_1 \bar{x}) \sum_{i=1}^n x_i - \beta_1 \sum_{i=1}^n x_i^2 = 0,
\]
so
\[
\sum_{i=1}^n x_i y_i - n\bar{y}\bar{x} = \beta_1 \left( \sum_{i=1}^n x_i^2 - n\bar{x}^2 \right).
\]
So
\[
\beta_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}.
\]

Thus, the least squares estimates are
\[
\hat{\beta}_1 = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}, \qquad
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}.
\]

These are unbiased estimators of the regression parameters. We do not
derive this result here; the derivation will be covered in the
second-year module, Statistical Modelling I).

## Estimating the variance parameter

In addition to estimating the regression parameters $\beta_0$ and $\beta_1$, we also need to estimate the variance parameter $\sigma^2$ of the error terms.

We estimate $\sigma^2$ with
\[
\hat{\sigma}^2 = \frac{1}{n-2} \sum_{i=1}^n (y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i)^2,
\]
where $\hat{\beta}_0$ and $\hat{\beta}_1$ are the least squares estimates.

This estimator is unbiased for $\sigma^2$. We do not derive this result here; the derivation will be covered in the second-year module, Statistical Modelling I.

## Model fitting in R

In practice, we do not need to apply these formulas by hand to find the
least squares estimates. Instead, we can use the `lm` function in `R`
to find the estimates. For instance, if the birth weight data is
stored in a data frame called `bw`, with columns `weight` for birth weight
(which is the response, $y$) and `ga` for gestational age (which is the covariate, $x$), 
then we can fit the simple linear regression model with:

```{r}
mod <- lm(weight ~ ga, data = bw)
```

We can inspect the fitted model:

```{r}
mod
```

We see $\hat \beta_0 =$ `r round(coef(mod)[1], 4)` 
and $\hat \beta_1 =$ `r round(coef(mod)[2], 4)`.

We can get more information about the model fit, including standard
errors for the estimates, by using the `summary` function in R:

```{r}
summary(mod)
```
You can learn more about expressing uncertainty in simple
linear regression the second-year module, Statistical Modelling I.

The "Residual standard error" (`r round(summary(mod)$sigma, 4)`) is an
estimate of $\sigma$. The variance estimate $\hat \sigma^2$ can be
found by squaring this number (in this case, $\hat \sigma^2 =$ 
`r round(summary(mod)$sigma^2, 4)`).

We can also plot the data with our fitted line overlaid:

```{r}
plot(weight ~ ga, data = bw)
abline(mod)
```

## Limitations of simple linear regression

While simple linear regression is a powerful and widely used tool, it has several important limitations:

- **Only one covariate:** Simple linear regression models the relationship between the response and a single covariate. In many real-world situations, multiple variables may influence the response.
- **Linearity assumption:** The model assumes that the relationship between the response and the covariate is linear. If the true relationship is non-linear, the model may not fit well.
- **Constant variance (homoscedasticity):** The errors are assumed to have constant variance across all values of the covariate. If the variance of the errors changes (heteroscedasticity), the model's estimates and inferences may be unreliable.
- **Normality of errors:** The errors are assumed to be normally distributed. If this assumption is violated, inference based on the model may not be valid.

These limitations, and methods to address them, will be covered in more detail in the second-year module, Statistical Modelling I.
