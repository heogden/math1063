<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introduction to Probability | MATH1063: Introduction to Statistics</title>
  <meta name="description" content="The course notes for MATH1063: Introduction to Statistics" />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introduction to Probability | MATH1063: Introduction to Statistics" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="The course notes for MATH1063: Introduction to Statistics" />
  <meta name="github-repo" content="heogden/math1063" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introduction to Probability | MATH1063: Introduction to Statistics" />
  
  <meta name="twitter:description" content="The course notes for MATH1063: Introduction to Statistics" />
  

<meta name="author" content="Dr Helen Ogden, based on original notes by Prof. Sujit Sahu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="distributions.html"/>
<script src="libs/jquery-3.6.1/jquery-3.6.1.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#what-is-statistics"><i class="fa fa-check"></i><b>1.1</b> What is statistics?</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="index.html"><a href="index.html#early-and-modern-definitions"><i class="fa fa-check"></i><b>1.1.1</b> Early and modern definitions</a></li>
<li class="chapter" data-level="1.1.2" data-path="index.html"><a href="index.html#statistics-tames-uncertainty"><i class="fa fa-check"></i><b>1.1.2</b> Statistics tames uncertainty</a></li>
<li class="chapter" data-level="1.1.3" data-path="index.html"><a href="index.html#why-should-i-study-statistics-as-part-of-my-degree"><i class="fa fa-check"></i><b>1.1.3</b> Why should I study statistics as part of my degree?</a></li>
<li class="chapter" data-level="1.1.4" data-path="index.html"><a href="index.html#lies-damn-lies-and-statistics"><i class="fa fa-check"></i><b>1.1.4</b> Lies, Damn Lies and Statistics?</a></li>
<li class="chapter" data-level="1.1.5" data-path="index.html"><a href="index.html#whats-in-this-module"><i class="fa fa-check"></i><b>1.1.5</b> What’s in this module?</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#example-data-sets"><i class="fa fa-check"></i><b>1.2</b> Example data sets</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#introduction-to-r"><i class="fa fa-check"></i><b>1.3</b> Introduction to R</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#summaries"><i class="fa fa-check"></i><b>1.4</b> Summarising data sets</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#summarising-categorical-data"><i class="fa fa-check"></i><b>1.4.1</b> Summarising categorical data</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#measures-of-location"><i class="fa fa-check"></i><b>1.4.2</b> Measures of location</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#measures-of-spread"><i class="fa fa-check"></i><b>1.4.3</b> Measures of spread</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#summarising-data-in-r"><i class="fa fa-check"></i><b>1.4.4</b> Summarising data in R</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#exploratory-data-plots"><i class="fa fa-check"></i><b>1.5</b> Exploratory data plots</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="index.html"><a href="index.html#introduction"><i class="fa fa-check"></i><b>1.5.1</b> Introduction</a></li>
<li class="chapter" data-level="1.5.2" data-path="index.html"><a href="index.html#distribution-of-a-single-discrete-variable"><i class="fa fa-check"></i><b>1.5.2</b> Distribution of a single discrete variable</a></li>
<li class="chapter" data-level="1.5.3" data-path="index.html"><a href="index.html#distribution-of-a-single-continuous-variable"><i class="fa fa-check"></i><b>1.5.3</b> Distribution of a single continuous variable</a></li>
<li class="chapter" data-level="1.5.4" data-path="index.html"><a href="index.html#relationship-between-continuous-and-discrete-variables"><i class="fa fa-check"></i><b>1.5.4</b> Relationship between continuous and discrete variables</a></li>
<li class="chapter" data-level="1.5.5" data-path="index.html"><a href="index.html#relationship-between-two-continuous-variables"><i class="fa fa-check"></i><b>1.5.5</b> Relationship between two continuous variables</a></li>
<li class="chapter" data-level="1.5.6" data-path="index.html"><a href="index.html#relationships-between-more-than-two-variables"><i class="fa fa-check"></i><b>1.5.6</b> Relationships between more than two variables</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro-prob.html"><a href="intro-prob.html"><i class="fa fa-check"></i><b>2</b> Introduction to Probability</a>
<ul>
<li class="chapter" data-level="2.1" data-path="intro-prob.html"><a href="intro-prob.html#definitions-of-probability"><i class="fa fa-check"></i><b>2.1</b> Definitions of probability</a></li>
<li class="chapter" data-level="2.2" data-path="intro-prob.html"><a href="intro-prob.html#some-definitions"><i class="fa fa-check"></i><b>2.2</b> Some definitions</a></li>
<li class="chapter" data-level="2.3" data-path="intro-prob.html"><a href="intro-prob.html#axioms-of-probability"><i class="fa fa-check"></i><b>2.3</b> Axioms of probability</a></li>
<li class="chapter" data-level="2.4" data-path="intro-prob.html"><a href="intro-prob.html#using-combinatorics-to-find-probabilities"><i class="fa fa-check"></i><b>2.4</b> Using combinatorics to find probabilities</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="intro-prob.html"><a href="intro-prob.html#experiments-with-equally-likely-outcomes"><i class="fa fa-check"></i><b>2.4.1</b> Experiments with equally likely outcomes</a></li>
<li class="chapter" data-level="2.4.2" data-path="intro-prob.html"><a href="intro-prob.html#sec-multiplication-rule"><i class="fa fa-check"></i><b>2.4.2</b> Multiplication rule of counting</a></li>
<li class="chapter" data-level="2.4.3" data-path="intro-prob.html"><a href="intro-prob.html#the-number-of-permutations-of-k-from-n-pn-k"><i class="fa fa-check"></i><b>2.4.3</b> The number of permutations of <span class="math inline">\(k\)</span> from <span class="math inline">\(n\)</span>: <span class="math inline">\(P(n, k)\)</span></a></li>
<li class="chapter" data-level="2.4.4" data-path="intro-prob.html"><a href="intro-prob.html#calculation-of-probabilities-of-events-under-sampling-at-random"><i class="fa fa-check"></i><b>2.4.4</b> Calculation of probabilities of events under sampling ‘at random’</a></li>
<li class="chapter" data-level="2.4.5" data-path="intro-prob.html"><a href="intro-prob.html#a-general-urn-problem"><i class="fa fa-check"></i><b>2.4.5</b> A general ‘urn problem’</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="intro-prob.html"><a href="intro-prob.html#conditional-probability-and-bayes-theorem"><i class="fa fa-check"></i><b>2.5</b> Conditional probability and Bayes’ Theorem</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="intro-prob.html"><a href="intro-prob.html#definition-of-conditional-probability"><i class="fa fa-check"></i><b>2.5.1</b> Definition of conditional probability</a></li>
<li class="chapter" data-level="2.5.2" data-path="intro-prob.html"><a href="intro-prob.html#multiplication-rule-of-conditional-probability"><i class="fa fa-check"></i><b>2.5.2</b> Multiplication rule of conditional probability</a></li>
<li class="chapter" data-level="2.5.3" data-path="intro-prob.html"><a href="intro-prob.html#total-probability-formula"><i class="fa fa-check"></i><b>2.5.3</b> Total probability formula</a></li>
<li class="chapter" data-level="2.5.4" data-path="intro-prob.html"><a href="intro-prob.html#bayes-theorem"><i class="fa fa-check"></i><b>2.5.4</b> Bayes’ theorem</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="intro-prob.html"><a href="intro-prob.html#independent-events"><i class="fa fa-check"></i><b>2.6</b> Independent events</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="intro-prob.html"><a href="intro-prob.html#introduction-and-definition-of-independence"><i class="fa fa-check"></i><b>2.6.1</b> Introduction and definition of independence</a></li>
<li class="chapter" data-level="2.6.2" data-path="intro-prob.html"><a href="intro-prob.html#independence-with-three-events"><i class="fa fa-check"></i><b>2.6.2</b> Independence with three events</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="distributions.html"><a href="distributions.html"><i class="fa fa-check"></i><b>3</b> Probability Distributions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="distributions.html"><a href="distributions.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="distributions.html"><a href="distributions.html#random-variables"><i class="fa fa-check"></i><b>3.2</b> Random variables</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="distributions.html"><a href="distributions.html#introduction-2"><i class="fa fa-check"></i><b>3.2.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2.2" data-path="distributions.html"><a href="distributions.html#discrete-and-continuous-random-variables"><i class="fa fa-check"></i><b>3.2.2</b> Discrete and continuous random variables</a></li>
<li class="chapter" data-level="3.2.3" data-path="distributions.html"><a href="distributions.html#probability-distribution-of-a-random-variable"><i class="fa fa-check"></i><b>3.2.3</b> Probability distribution of a random variable</a></li>
<li class="chapter" data-level="3.2.4" data-path="distributions.html"><a href="distributions.html#continuous-random-variables"><i class="fa fa-check"></i><b>3.2.4</b> Continuous random variables</a></li>
<li class="chapter" data-level="3.2.5" data-path="distributions.html"><a href="distributions.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>3.2.5</b> Cumulative distribution function (cdf)</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="distributions.html"><a href="distributions.html#summaries-of-a-random-variable"><i class="fa fa-check"></i><b>3.3</b> Summaries of a random variable</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="distributions.html"><a href="distributions.html#introduction-3"><i class="fa fa-check"></i><b>3.3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.3.2" data-path="distributions.html"><a href="distributions.html#expectation"><i class="fa fa-check"></i><b>3.3.2</b> Expectation</a></li>
<li class="chapter" data-level="3.3.3" data-path="distributions.html"><a href="distributions.html#variance"><i class="fa fa-check"></i><b>3.3.3</b> Variance</a></li>
<li class="chapter" data-level="3.3.4" data-path="distributions.html"><a href="distributions.html#quantiles"><i class="fa fa-check"></i><b>3.3.4</b> Quantiles</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="distributions.html"><a href="distributions.html#standard-discrete-distributions"><i class="fa fa-check"></i><b>3.4</b> Standard discrete distributions</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="distributions.html"><a href="distributions.html#bernoulli-distribution"><i class="fa fa-check"></i><b>3.4.1</b> Bernoulli distribution</a></li>
<li class="chapter" data-level="3.4.2" data-path="distributions.html"><a href="distributions.html#binomial-distribution"><i class="fa fa-check"></i><b>3.4.2</b> Binomial distribution</a></li>
<li class="chapter" data-level="3.4.3" data-path="distributions.html"><a href="distributions.html#geometric-distribution"><i class="fa fa-check"></i><b>3.4.3</b> Geometric distribution</a></li>
<li class="chapter" data-level="3.4.4" data-path="distributions.html"><a href="distributions.html#hypergeometric-distribution"><i class="fa fa-check"></i><b>3.4.4</b> Hypergeometric distribution</a></li>
<li class="chapter" data-level="3.4.5" data-path="distributions.html"><a href="distributions.html#negative-binomial-distribution"><i class="fa fa-check"></i><b>3.4.5</b> Negative binomial distribution</a></li>
<li class="chapter" data-level="3.4.6" data-path="distributions.html"><a href="distributions.html#poisson-distribution"><i class="fa fa-check"></i><b>3.4.6</b> Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="distributions.html"><a href="distributions.html#standard-continuous-distributions"><i class="fa fa-check"></i><b>3.5</b> Standard continuous distributions</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="distributions.html"><a href="distributions.html#uniform-distribution"><i class="fa fa-check"></i><b>3.5.1</b> Uniform distribution</a></li>
<li class="chapter" data-level="3.5.2" data-path="distributions.html"><a href="distributions.html#exponential-distribution"><i class="fa fa-check"></i><b>3.5.2</b> Exponential distribution</a></li>
<li class="chapter" data-level="3.5.3" data-path="distributions.html"><a href="distributions.html#normal-distribution"><i class="fa fa-check"></i><b>3.5.3</b> Normal distribution</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="distributions.html"><a href="distributions.html#joint-distributions"><i class="fa fa-check"></i><b>3.6</b> Joint distributions</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="distributions.html"><a href="distributions.html#introduction-4"><i class="fa fa-check"></i><b>3.6.1</b> Introduction</a></li>
<li class="chapter" data-level="3.6.2" data-path="distributions.html"><a href="distributions.html#joint-distribution-of-discrete-random-variables"><i class="fa fa-check"></i><b>3.6.2</b> Joint distribution of discrete random variables</a></li>
<li class="chapter" data-level="3.6.3" data-path="distributions.html"><a href="distributions.html#joint-distribution-of-continuous-random-variables"><i class="fa fa-check"></i><b>3.6.3</b> Joint distribution of continuous random variables</a></li>
<li class="chapter" data-level="3.6.4" data-path="distributions.html"><a href="distributions.html#covariance-and-correlation"><i class="fa fa-check"></i><b>3.6.4</b> Covariance and correlation</a></li>
<li class="chapter" data-level="3.6.5" data-path="distributions.html"><a href="distributions.html#sec:indep"><i class="fa fa-check"></i><b>3.6.5</b> Independence</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="distributions.html"><a href="distributions.html#sec:sum-rvs"><i class="fa fa-check"></i><b>3.7</b> Sums of random variables</a></li>
<li class="chapter" data-level="3.8" data-path="distributions.html"><a href="distributions.html#sec:clt"><i class="fa fa-check"></i><b>3.8</b> The Central Limit Theorem</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="distributions.html"><a href="distributions.html#introduction-5"><i class="fa fa-check"></i><b>3.8.1</b> Introduction</a></li>
<li class="chapter" data-level="3.8.2" data-path="distributions.html"><a href="distributions.html#statement-of-the-central-limit-theorem-clt"><i class="fa fa-check"></i><b>3.8.2</b> Statement of the Central Limit Theorem (CLT)</a></li>
<li class="chapter" data-level="3.8.3" data-path="distributions.html"><a href="distributions.html#application-of-clt-to-binomial-distribution"><i class="fa fa-check"></i><b>3.8.3</b> Application of CLT to binomial distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>4</b> Statistical Inference</a>
<ul>
<li class="chapter" data-level="4.1" data-path="inference.html"><a href="inference.html#statistical-modelling"><i class="fa fa-check"></i><b>4.1</b> Statistical modelling</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="inference.html"><a href="inference.html#introduction-6"><i class="fa fa-check"></i><b>4.1.1</b> Introduction</a></li>
<li class="chapter" data-level="4.1.2" data-path="inference.html"><a href="inference.html#statistical-models"><i class="fa fa-check"></i><b>4.1.2</b> Statistical models</a></li>
<li class="chapter" data-level="4.1.3" data-path="inference.html"><a href="inference.html#a-fully-specified-model"><i class="fa fa-check"></i><b>4.1.3</b> A fully specified model</a></li>
<li class="chapter" data-level="4.1.4" data-path="inference.html"><a href="inference.html#a-parametric-statistical-model"><i class="fa fa-check"></i><b>4.1.4</b> A parametric statistical model</a></li>
<li class="chapter" data-level="4.1.5" data-path="inference.html"><a href="inference.html#a-nonparametric-statistical-model"><i class="fa fa-check"></i><b>4.1.5</b> A nonparametric statistical model</a></li>
<li class="chapter" data-level="4.1.6" data-path="inference.html"><a href="inference.html#should-we-prefer-parametric-or-nonparametric-and-why"><i class="fa fa-check"></i><b>4.1.6</b> Should we prefer parametric or nonparametric and why?</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="inference.html"><a href="inference.html#estimation"><i class="fa fa-check"></i><b>4.2</b> Estimation</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="inference.html"><a href="inference.html#introduction-7"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="inference.html"><a href="inference.html#population-and-sample"><i class="fa fa-check"></i><b>4.2.2</b> Population and sample</a></li>
<li class="chapter" data-level="4.2.3" data-path="inference.html"><a href="inference.html#statistic-and-estimator"><i class="fa fa-check"></i><b>4.2.3</b> Statistic and estimator</a></li>
<li class="chapter" data-level="4.2.4" data-path="inference.html"><a href="inference.html#bias-and-mean-square-error"><i class="fa fa-check"></i><b>4.2.4</b> Bias and mean square error</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="inference.html"><a href="inference.html#estimating-the-population-mean"><i class="fa fa-check"></i><b>4.3</b> Estimating the population mean</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="inference.html"><a href="inference.html#introduction-8"><i class="fa fa-check"></i><b>4.3.1</b> Introduction</a></li>
<li class="chapter" data-level="4.3.2" data-path="inference.html"><a href="inference.html#estimation-of-a-population-mean"><i class="fa fa-check"></i><b>4.3.2</b> Estimation of a population mean</a></li>
<li class="chapter" data-level="4.3.3" data-path="inference.html"><a href="inference.html#standard-deviation-and-standard-error"><i class="fa fa-check"></i><b>4.3.3</b> Standard deviation and standard error</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>4.4</b> Confidence intervals</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="inference.html"><a href="inference.html#introduction-9"><i class="fa fa-check"></i><b>4.4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.4.2" data-path="inference.html"><a href="inference.html#confidence-interval-for-a-normal-mean"><i class="fa fa-check"></i><b>4.4.2</b> Confidence interval for a normal mean</a></li>
<li class="chapter" data-level="4.4.3" data-path="inference.html"><a href="inference.html#some-remarks-about-confidence-intervals"><i class="fa fa-check"></i><b>4.4.3</b> Some remarks about confidence intervals</a></li>
<li class="chapter" data-level="4.4.4" data-path="inference.html"><a href="inference.html#confidence-intervals-using-the-clt"><i class="fa fa-check"></i><b>4.4.4</b> Confidence intervals using the CLT</a></li>
<li class="chapter" data-level="4.4.5" data-path="inference.html"><a href="inference.html#exact-confidence-interval-for-the-normal-mean"><i class="fa fa-check"></i><b>4.4.5</b> Exact confidence interval for the normal mean</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="inference.html"><a href="inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.5</b> Hypothesis testing</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="inference.html"><a href="inference.html#hypothesis-testing-in-general"><i class="fa fa-check"></i><b>4.5.1</b> Hypothesis testing in general</a></li>
<li class="chapter" data-level="4.5.2" data-path="inference.html"><a href="inference.html#testing-a-normal-mean-t-test"><i class="fa fa-check"></i><b>4.5.2</b> Testing a normal mean (t-test)</a></li>
<li class="chapter" data-level="4.5.3" data-path="inference.html"><a href="inference.html#two-sample-t-tests"><i class="fa fa-check"></i><b>4.5.3</b> Two sample t-tests</a></li>
<li class="chapter" data-level="4.5.4" data-path="inference.html"><a href="inference.html#paired-t-test"><i class="fa fa-check"></i><b>4.5.4</b> Paired t-test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="lm.html"><a href="lm.html"><i class="fa fa-check"></i><b>5</b> Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="lm.html"><a href="lm.html#what-is-regression"><i class="fa fa-check"></i><b>5.1</b> What is regression?</a></li>
<li class="chapter" data-level="5.2" data-path="lm.html"><a href="lm.html#simple-linear-regression"><i class="fa fa-check"></i><b>5.2</b> Simple linear regression</a></li>
<li class="chapter" data-level="5.3" data-path="lm.html"><a href="lm.html#estimating-the-regression-parameters"><i class="fa fa-check"></i><b>5.3</b> Estimating the regression parameters</a></li>
<li class="chapter" data-level="5.4" data-path="lm.html"><a href="lm.html#model-fitting-in-r"><i class="fa fa-check"></i><b>5.4</b> Model fitting in R</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH1063: Introduction to Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intro-prob" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Introduction to Probability<a href="intro-prob.html#intro-prob" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="definitions-of-probability" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Definitions of probability<a href="intro-prob.html#definitions-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Probabilities are often used to express the uncertainty of events of interest happening. For example,
we may say that: (i) I think
it is highly likely that Manchester City will retain the premiership title this season
or to be more specific, I think there is more than an 80% chance that Manchester City
will keep the title;
(ii) the probability of a tossed fair coin landing heads is 0.5.
As we have seen in Chapter <a href="index.html#intro-stats">1</a>, there is uncertainty
everywhere, and probability is the language we use to quantify this uncertainty.</p>
<p>The two examples above, football and tossing a coin, convey two different interpretations of
probability. The football probability is the commentator’s own subjective belief. The
commentator certainly has not performed a large experiment involving all the 20 teams over the
whole (future) season under all playing conditions, players, managers and transfers. This notion
is known as subjective probability. Subjective probability gives a measure of the plausibility of
the proposition, to the person making it, in the light of past experience (e.g. Manchester City are the
current champions) and other evidence (e.g. they spent the maximum amount of money buying
players). There are plenty of other examples, e.g. I think there is a 70% chance that the FTSE
100 will rise tomorrow, or according to the Met Office there is a 40% chance that we will have a
white Christmas this year in Southampton.</p>
<p>The second definition of probability comes from the long-term relative frequency of a result of
a random experiment (e.g. coin tossing) which can be repeated an infinite number of times under
essentially similar conditions.</p>
<p>Imagine we are able to repeat a random experiment
under identical conditions and count how many of those repetitions result in the event <span class="math inline">\(A\)</span>. The
relative frequency of <span class="math inline">\(A\)</span>, i.e. the ratio
<span class="math display">\[\frac{\text{the number of repetitions resulting in $A$}}{\text{total number of repetitions}},\]</span>
approaches a fixed limit value as the number of repetitions increases. This limit value is defined as
<span class="math inline">\(P\{A\}\)</span>.</p>
<p>As a simple example, in the experiment of tossing a particular coin, suppose we are interested
in the event <span class="math inline">\(A\)</span> of getting a ‘head’. We can toss the coin 1000 times (i.e. do 1000 replications of
the experiment) and record the number of heads out of the 1000 replications. Then the relative
frequency of <span class="math inline">\(A\)</span> out of the 1000 replications is the proportion of heads observed.</p>
<p>Sometimes, however, it is much easier to find <span class="math inline">\(P\{A\}\)</span> by using some ‘common knowledge’ about
probability. For example, if the coin in the example above is fair
(i.e. <span class="math inline">\(P\{\text{head}\} = P\{\text{tail}\})\)</span>, then
this information and the common knowledge that <span class="math inline">\(P\{\text{head}\}+P\{\text{tail}\} = 1\)</span>
immediately imply that
<span class="math inline">\(P\{\text{head}\} = 0.5\)</span> and <span class="math inline">\(P\{\text{tail}\} = 0.5\)</span>.
The ‘common knowledge’ about probability
will be formalised as the axioms of probability, which form the foundation of probability theory.</p>
</div>
<div id="some-definitions" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Some definitions<a href="intro-prob.html#some-definitions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we can state and use the axioms of probability,
we need introduce some terminology.</p>
<p>A <em>random experiment</em> is one in which we do not know exactly
what outcome the experiment will give, even though we can write down all the possible outcomes.
The set of all possible outcomes is
called the <em>sample space</em> (<span class="math inline">\(S\)</span>). For example, in a coin tossing experiment, <span class="math inline">\(S = \{\text{head}, \text{tail}\}\)</span>. If we toss two coins together,
<span class="math inline">\(S = \{\text{HH}, \text{HT}, \text{TH}, \text{TT}\}\)</span> where <span class="math inline">\(\text{H}\)</span> and <span class="math inline">\(\text{T}\)</span> denote
respectively the outcome head and tail from the toss of a single coin.</p>
<p>An <em>event</em> is a particular result of the random experiment. For example,
<span class="math inline">\(\text{HH}\)</span>
(two heads) is an event when we toss two coins together. Similarly, at least one head e.g. 
<span class="math inline">\(\{\text{HH}, \text{HT}, \text{TH}\}\)</span> is an event as well.
Events are denoted by capital letters <span class="math inline">\(A, B, C, \ldots\)</span> or <span class="math inline">\(A_1, B_1, A_2\)</span> etc., and
a single outcome is called an <em>elementary event</em>, e.g. <span class="math inline">\(\text{HH}\)</span>.
An event which is a group of elementary
events is called a <em>composite event</em>, e.g. at least one head. How to determine the probability of a
given event <span class="math inline">\(A\)</span>, <span class="math inline">\(P\{A\}\)</span>, is the focus of probability theory.</p>
<div class="example">
<p><span id="exm:die-throw" class="example"><strong>Example 2.1  (Die throw) </strong></span>Roll a six-faced die and observe the score on the uppermost face.
Here <span class="math inline">\(S = \{1, 2, 3, 4, 5, 6\}\)</span>, which is composed of six elementary events.</p>
</div>
<p>The <em>union</em> of two given events <span class="math inline">\(A\)</span> and B, denoted as (<span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span>) or <span class="math inline">\(A \cup B\)</span>,
consists of the outcomes
that are either in A or B or both. ‘Event <span class="math inline">\(A \cup B\)</span> occurs’ means
‘either <span class="math inline">\(A\)</span> or <span class="math inline">\(B\)</span> occurs or both occur’.</p>
<p>For example, in Example <a href="intro-prob.html#exm:die-throw">2.1</a>,
suppose <span class="math inline">\(A\)</span> is the event that <em>an even number is observed</em>. This
event consists of the set of outcomes 2, 4 and 6, i.e. <span class="math inline">\(A = \{\text{an even number}\} = \{2, 4, 6\}\)</span>.
Suppose <span class="math inline">\(B\)</span> is the event that <em>a number larger than 3 is observed</em>. This event consists of the
outcomes 4, 5 and 6, i.e. <span class="math inline">\(B = \{\text{a number larger than 3}\} = \{4, 5, 6\}\)</span>.
Hence the event <span class="math inline">\(A \cup B = \{\text{an even number or a number larger than 3}\} = \{2, 4, 5, 6\}\)</span>.
Clearly, when a <span class="math inline">\(6\)</span> is observed, both <span class="math inline">\(A\)</span>
and <span class="math inline">\(B\)</span> have occurred.</p>
<p>The <em>intersection</em> of two given events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, denoted as (<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>) or <span class="math inline">\(A \cap B\)</span>,
consists of the
outcomes that are common to both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>. ‘Event <span class="math inline">\(A \cap B\)</span> occurs’ means ‘both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> occur’. For
example, in Example <a href="intro-prob.html#exm:die-throw">2.1</a>,
<span class="math inline">\(A \cap B = \{4, 6\}\)</span>. Additionally, if <span class="math inline">\(C = \{\text{a number less than 6}\} = \{1, 2, 3, 4, 5\}\)</span>,
the intersection of events <span class="math inline">\(A\)</span> and <span class="math inline">\(C\)</span> is the event <span class="math inline">\(A \cap C = \{\text{an even number less than 6}\} = \{2, 4\}\)</span>.</p>
<p>The union and intersection of two events can be generalized in an obvious way to the union and
intersection of more than two events.</p>
<p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> are said to be <em>mutually exclusive</em> if <span class="math inline">\(A \cap D = \emptyset\)</span>, where <span class="math inline">\(\emptyset\)</span> denotes the empty
set, i.e. <span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> have no outcomes in common. Intuitively, ‘<span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> are mutually exclusive’
means ‘<span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> cannot occur simultaneously in the experiment’.</p>
<p>In Example <a href="intro-prob.html#exm:die-throw">2.1</a>,
if <span class="math inline">\(D = \{\text{an odd number}\} = \{1, 3, 5\}\)</span>, then <span class="math inline">\(A \cap D = \emptyset\)</span> and so <span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> are
mutually exclusive. As expected, <span class="math inline">\(A\)</span> and <span class="math inline">\(D\)</span> cannot occur simultaneously in the experiment.</p>
<p>For a given event <span class="math inline">\(A\)</span>, the <em>complement</em> of <span class="math inline">\(A\)</span> is the event that consists of all the outcomes not in
<span class="math inline">\(A\)</span> and is denoted by <span class="math inline">\(A^\prime\)</span> . Note that <span class="math inline">\(A \cup A^\prime = S\)</span> and <span class="math inline">\(A \cap A^\prime = \emptyset\)</span>.</p>
</div>
<div id="axioms-of-probability" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Axioms of probability<a href="intro-prob.html#axioms-of-probability" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Here are the three axioms of probability:</p>
<p><strong>A1</strong> <span class="math inline">\(P\{S\}=1\)</span>.<br />
<strong>A2</strong> <span class="math inline">\(0 \leq P\{A\} \leq 1\)</span> for any event <span class="math inline">\(A\)</span>.<br />
<strong>A3</strong> <span class="math inline">\(P\{A \cup B\}=P\{A\}+P\{B\}\)</span> provided that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are mutually exclusive events.</p>
<p>Here are some of the consequences of the axioms of probability:</p>
<ol style="list-style-type: decimal">
<li>For any event <span class="math inline">\(A, P\{A\}=1-P\left\{A^{\prime}\right\}\)</span>.</li>
<li>From (1) and Axiom A1, <span class="math inline">\(P\{\emptyset\}=1-P\{S\}=0\)</span>. Hence if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are mutually exclusive events, then <span class="math inline">\(P\{A \cap B\}=0\)</span>.</li>
<li>If <span class="math inline">\(D\)</span> is a subset of <span class="math inline">\(E, D \subset E\)</span>, then <span class="math inline">\(P\left\{E \cap D^{\prime}\right\}=P\{E\}-P\{D\}\)</span> which implies for arbitrary events <span class="math inline">\(A\)</span> and <span class="math inline">\(B, P\left\{A \cap B^{\prime}\right\}=P\{A\}-P\{A \cap B\}\)</span>.</li>
<li>It can be shown by mathematical induction that Axiom A3 holds for more than two mutually exclusive events:
<span class="math display">\[
P\left\{A_{1} \cup A_{2} \cup \cdots \cup A_{k}\right\}=P\left\{A_{1}\right\}+P\left\{A_{2}\right\}+\ldots+P\left\{A_{k}\right\}
\]</span>
provided that <span class="math inline">\(A_{1}, \ldots, A_{k}\)</span> are mutually exclusive events.
Hence, the probability of an event A is the sum of the probabilities of the individual outcomes that make up the event.</li>
<li>For the union of two arbitrary events, we have the General addition rule: For any two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>
<span class="math display">\[
P\{A \cup B\}=P\{A\}+P\{B\}-P\{A \cap B\} .
\]</span></li>
</ol>
<div class="proof">
<p><span id="unlabeled-div-4" class="proof"><em>Proof</em>. </span>We can write <span class="math inline">\(A \cup B=\left(A \cap B^{\prime}\right) \cup(A \cap B) \cup\left(A^{\prime} \cap B\right)\)</span>. All three of these are mutually exclusive events. Hence,
<span class="math display">\[
\begin{aligned}
P\{A \cup B\} &amp;=P\left\{A \cap B^{\prime}\right\}+P\{A \cap B\}+P\left\{A^{\prime} \cap B\right\} \\
&amp;=P\{A\}-P\{A \cap B\}+P\{A \cap B\}+P\{B\}-P\{A \cap B\} \\
&amp;=P\{A\}+P\{B\}-P\{A \cap B\}.
\end{aligned}
\]</span></p>
</div>
<ol start="6" style="list-style-type: decimal">
<li>The sum of the probabilities of all the outcomes in the sample space <span class="math inline">\(S\)</span> is 1 .</li>
</ol>
</div>
<div id="using-combinatorics-to-find-probabilities" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Using combinatorics to find probabilities<a href="intro-prob.html#using-combinatorics-to-find-probabilities" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="experiments-with-equally-likely-outcomes" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Experiments with equally likely outcomes<a href="intro-prob.html#experiments-with-equally-likely-outcomes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For an experiment with <span class="math inline">\(N\)</span> equally likely possible outcomes, the axioms (and the consequences
above) can be used to find <span class="math inline">\(P\{A\}\)</span> of any event <span class="math inline">\(A\)</span> in the following way.</p>
<p>From consequence (4), we assign probability <span class="math inline">\(1/N\)</span> to each outcome.</p>
<p>For any event <span class="math inline">\(A\)</span>, we find <span class="math inline">\(P\{A\}\)</span> by adding up <span class="math inline">\(1/N\)</span> for each of the outcomes in event <span class="math inline">\(A\)</span>:
<span class="math display">\[P \{A\} = \frac{\text{number of outcomes in $A$}}
{\text{total number of possible outcomes of the experiment}}.\]</span></p>
<p>For experiments with equally likely outcomes, the task of calculating the probability
of an event therefore reduces to counting the number of outcomes in the event
and the total number of possible outcomes.
In the following sections, will use ideas from combinatorics
(the mathematics of counting) to help us complete these tasks.</p>
<p>Return to Example <a href="intro-prob.html#exm:die-throw">2.1</a>
where a six-faced die is rolled. Suppose that one wins a bet if a 6 is
rolled. Then the probability of winning the bet is <span class="math inline">\(1/6\)</span> as there are six possible outcomes in the
sample space and exactly one of those, 6, wins the bet. Suppose <span class="math inline">\(A\)</span> denotes the event that an
even-numbered face is rolled. Then <span class="math inline">\(P\{A\} = 3/6 = 1/2\)</span> as we can expect.</p>
<div class="example">
<p><span id="exm:dice" class="example"><strong>Example 2.2  (Dice throw) </strong></span>Roll 2 distinguishable dice and observe the scores. Here
<span class="math display">\[S =
\{(1, 1), (1, 2), (1, 3), (1, 4), (1, 5), (1, 6), . . . , (6, 1), (6, 2), (6, 3), (6, 4), (6, 5), (6, 6)\}\]</span>
which consists of 36
possible outcomes or elementary events, <span class="math inline">\(A_1, \ldots , A_{36}\)</span>.
What is the probability of the outcome <span class="math inline">\(6\)</span> in
both the dice? The required probability is <span class="math inline">\(1/36\)</span>. What is the probability that the sum of the two
dice is greater than 6? How about the probability that the sum is less than any number, e.g. 8?</p>
<p><strong>Hint</strong>: Write down the sum for each of the 36 outcomes and then find the probabilities asked just
by inspection. Remember, each of the 36 outcomes has equal probability <span class="math inline">\(1/36\)</span>.</p>
</div>
</div>
<div id="sec-multiplication-rule" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> Multiplication rule of counting<a href="intro-prob.html#sec-multiplication-rule" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To complete a specific task, suppose one has to complete <span class="math inline">\(k(\geq 1)\)</span> sub-tasks sequentially.
If there are
<span class="math inline">\(n_i\)</span>
different ways to complete the <span class="math inline">\(i\)</span>th sub-task (<span class="math inline">\(i = 1, \ldots, k\)</span>) then there are
<span class="math inline">\(n_1 \times n_2 \times \dots \times n_k\)</span> different
ways to complete the task.</p>
<div class="example">
<p><span id="exm:unlabeled-div-5" class="example"><strong>Example 2.3  (Bus routes) </strong></span>Suppose there are 7 routes to London from Southampton and
then there are 5 routes to Cambridge out of London. How many ways can
I travel to Cambridge from Southampton via London? The answer is
obviously 35.</p>
</div>
</div>
<div id="the-number-of-permutations-of-k-from-n-pn-k" class="section level3 hasAnchor" number="2.4.3">
<h3><span class="header-section-number">2.4.3</span> The number of permutations of <span class="math inline">\(k\)</span> from <span class="math inline">\(n\)</span>: <span class="math inline">\(P(n, k)\)</span><a href="intro-prob.html#the-number-of-permutations-of-k-from-n-pn-k" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose we are asked to select <span class="math inline">\(k(\geq 1)\)</span> from the <span class="math inline">\(n (n \geq k)\)</span> available people and sit the
<span class="math inline">\(k\)</span> selected people in <span class="math inline">\(k\)</span>
(different) chairs. How many different ways are there to complete the task?</p>
<p>By considering the <span class="math inline">\(i\)</span>th sub-task as selecting a person to sit in
the <span class="math inline">\(i\)</span>th chair (<span class="math inline">\(i = 1, \ldots, k\)</span>), it follows directly from the multiplication rule
in Section <a href="intro-prob.html#sec-multiplication-rule">2.4.2</a>
that there are <span class="math inline">\(n(n-1) \ldots (n-[k -1])\)</span>
ways to complete the task.</p>
<p>The number <span class="math inline">\(n(n-1) \ldots (n-[k-1])\)</span>
is called the number of permutations
of <span class="math inline">\(k\)</span> from <span class="math inline">\(n\)</span> and denoted by
<span class="math display">\[P(n, k) = n(n - 1) \ldots (n - [k - 1]).\]</span>
In particular, when <span class="math inline">\(k = n\)</span> we have <span class="math inline">\(P(n, n) = n(n - 1) \ldots 1\)</span>, which
is called ‘<span class="math inline">\(n\)</span> factorial’ and denoted
as <span class="math inline">\(n!\)</span>. Note that <span class="math inline">\(0!\)</span> is defined to be <span class="math inline">\(1\)</span>. We have
<span class="math display">\[P(n, k)=n(n-1) \ldots(n-[k-1])=\frac{n(n-1) \ldots(n-[k-1]) \times(n-k) !}{(n-k) !}=\frac{n !}{(n-k) !}.\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-6" class="example"><strong>Example 2.4  (Football) </strong></span>How many possible rankings are there for the 20 football teams in
the premier league at the end of a season? This number is given by <span class="math inline">\(P(20, 20) = 20!\)</span>, which is a huge
number! How many possible permutations are there for the top 4 positions who will qualify to play
in Europe in the next season? This number is given by <span class="math inline">\(P(20, 4) = 20 \times 19 \times 18 \times 17\)</span>.</p>
</div>
<div id="the-number-of-combinations-of-k-from-n-binomnk" class="section level4 hasAnchor" number="2.4.3.1">
<h4><span class="header-section-number">2.4.3.1</span> The number of combinations of <span class="math inline">\(k\)</span> from <span class="math inline">\(n\)</span>: <span class="math inline">\(\binom{n}{k}\)</span><a href="intro-prob.html#the-number-of-combinations-of-k-from-n-binomnk" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Suppose we are asked to select <span class="math inline">\(k(\geq 1)\)</span> from the <span class="math inline">\(n\)</span> (<span class="math inline">\(n \geq k\)</span>) available people.
Note that this task does NOT
involve sitting the <span class="math inline">\(k\)</span> selected people in <span class="math inline">\(k\)</span> (different) chairs.
We want to find the number of possible
ways to complete this task, which is denoted as <span class="math inline">\(\binom{n}{k}\)</span>.</p>
<p>For this, let us reconsider the task of “selecting <span class="math inline">\(k\)</span> from the <span class="math inline">\(n\)</span> available people and
sitting the <span class="math inline">\(k\)</span> selected people in <span class="math inline">\(k\)</span> (different) chairs”, which we already know from the discussion
above has <span class="math inline">\(P(n, k)\)</span> ways to complete.
Alternatively, to complete this task, one has to complete two sub-tasks sequentially. The first
sub-task is to select <span class="math inline">\(k\)</span> from the <span class="math inline">\(n\)</span> available people, which has <span class="math inline">\(\binom{n}{k}\)</span>
ways. The second
sub-task is to sit the <span class="math inline">\(k\)</span> selected people in <span class="math inline">\(k\)</span> (different) chairs, which has <span class="math inline">\(k!\)</span> ways.
It follows directly
from the multiplication rule that there are <span class="math inline">\(\binom{n}{k} \times k!\)</span> ways
to complete the task. Hence we have
<span class="math display">\[ P(n, k)= \binom{n}{k} \times k!\]</span>
so
<span class="math display">\[\binom{n}{k} =\frac{P(n, k)}{k!}=\frac{n!}{(n-k)! k!}.\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-7" class="example"><strong>Example 2.5  (Football) </strong></span>How many possible ways are there to choose 3 teams for the bottom
positions of the premier league table at the end of a season? This number is given by
<span class="math inline">\(\binom{20}{3} = 20 \times 19 \times 18/3!\)</span>, which does not take into consideration the rankings of the three bottom teams.</p>
</div>
<div class="example">
<p><span id="exm:microchip" class="example"><strong>Example 2.6  (Microchips) </strong></span>A box contains 12 microchips of which 4 are faulty. A sample of size
3 is drawn from the box without replacement.</p>
<ul>
<li>How many selections of 3 can be made? <span class="math inline">\(\binom{12}{3}\)</span></li>
<li>How many samples have all 3 chips faulty? <span class="math inline">\(\binom{4}{3}\)</span>.</li>
<li>How many selections have exactly 2 faulty chips? <span class="math inline">\(\binom{8}{1} \binom{4}{2}\)</span></li>
<li>How many samples of 3 have 2 or more faulty chips? <span class="math inline">\(\binom{8}{1} \binom{4}{2} + \binom{4}{3}\)</span></li>
</ul>
</div>
</div>
</div>
<div id="calculation-of-probabilities-of-events-under-sampling-at-random" class="section level3 hasAnchor" number="2.4.4">
<h3><span class="header-section-number">2.4.4</span> Calculation of probabilities of events under sampling ‘at random’<a href="intro-prob.html#calculation-of-probabilities-of-events-under-sampling-at-random" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For the experiment of ‘selecting a sample of size <span class="math inline">\(n\)</span> from a box of <span class="math inline">\(N\)</span> items without replacement’,
a sample is said to be selected at random if all the possible samples of size n are equally likely to
be selected. All the possible samples are then equally likely outcomes of the experiment and so
assigned equal probabilities.</p>
<div class="example">
<p><span id="exm:unlabeled-div-8" class="example"><strong>Example 2.7  (Microchips continued) </strong></span>In Example <a href="intro-prob.html#exm:microchip">2.6</a> assume that 3 microchips are selected at
random without replacement. Then</p>
<ul>
<li>each outcome (sample of size <span class="math inline">\(3\)</span>) has probability <span class="math inline">\(1/\binom{12}{3}\)</span>.</li>
<li><span class="math inline">\(P\{\text{all 3 selected microchips are faulty}\} = \binom{4}{3}/ \binom{12}{3}\)</span>.</li>
<li><span class="math inline">\(P\{\text{2 chips are faulty}\} = \binom{8}{1} \binom{4}{2}/ \binom{12}{3}\)</span>.</li>
<li><span class="math inline">\(P\{\text{2 or more chips are faulty}\} = \big(\binom{8}{1} \binom{4}{2} + \binom{4}{3}\big)/\binom{12}{3}.\)</span></li>
</ul>
</div>
</div>
<div id="a-general-urn-problem" class="section level3 hasAnchor" number="2.4.5">
<h3><span class="header-section-number">2.4.5</span> A general ‘urn problem’<a href="intro-prob.html#a-general-urn-problem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Example <a href="intro-prob.html#exm:microchip">2.6</a> is one particular case of the following general
urn problem which can be solved by the
same technique.
A sample of size <span class="math inline">\(n\)</span> is drawn at random without replacement from a box of <span class="math inline">\(N\)</span> items containing
a proportion <span class="math inline">\(p\)</span> of defective items.</p>
<ul>
<li>How many defective items are in the box? <span class="math inline">\(N p\)</span>. How many good items are there? <span class="math inline">\(N (1 - p)\)</span>.
Assume these to be integers.</li>
<li>The probability of exactly <span class="math inline">\(x\)</span> defective items in the sample of <span class="math inline">\(n\)</span> items is
<span class="math display">\[\frac{\binom{Np}{x} \binom{N(1-p)}{n-x}}{\binom{N}{n}}.\]</span></li>
<li>Which values of <span class="math inline">\(x\)</span> (in terms of <span class="math inline">\(N\)</span>, <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>) make this expression well defined?
We’ll see later that these values of <span class="math inline">\(x\)</span> and the corresponding probabilities make up what is called
the <em>hyper-geometric</em> distribution.</li>
</ul>
<div class="example">
<p><span id="exm:committee" class="example"><strong>Example 2.8  (Selecting a committee) </strong></span>There are 10 students available for a committee of
which 4 are men and 6 are women. A random sample of 3 students are chosen to form the committee
— what is the probability that exactly one is a man?
The total number of possible outcomes of the experiment is equal to the number of ways of
selecting 3 students from 10 and is given by <span class="math inline">\(\binom{10}{3}\)</span>.
The number of outcomes in the event ‘exactly one
is a man’ is equal to the number of ways of selecting 3 students from 10 with exactly one man, and
given by <span class="math inline">\(\binom{4}{1} \binom{6}{2}\)</span>
Hence
<span class="math display">\[\begin{align*}
P \{\text{exactly one man}\} &amp;= \frac{\text{number of ways of selecting one man and two women}}
{\text{number of ways of selecting 3 students}} \\
&amp;= \frac{\binom{4}{1} \binom{6}{2}}{\binom{10}{3}} = \frac{4 \times 15}{120} = \frac{1}{2}
\end{align*}\]</span></p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-9" class="example"><strong>Example 2.9  (The National Lottery) </strong></span>In Lotto, a winning ticket has six numbers from 1 to 59
matching those on the balls drawn on a Wednesday or Saturday evening. The ‘experiment’ consists
of drawing the balls from a box containing 59 balls. The ‘randomness’, the equal chance of any set
of six numbers being drawn, is ensured by the spinning machine, which rotates the balls during the
selection process. What is the probability of winning the jackpot?
The total number of possible selections of six balls/numbers is <span class="math inline">\(\binom{59}{6}\)</span>
There is only 1 selection for winning the jackpot. Hence
<span class="math display">\[P\{\text{jackpot}\} = \frac{1}{\binom{59}{6}} = 2.22 \times 10^{-8},\]</span>
which is roughly 1 in 45 million.</p>
<p>There is one other way of win a very large prize, of £1 million,
by using the bonus ball — matching 5 of the selected 6 balls
plus matching the bonus ball. The probability of this is given by
<span class="math display">\[P \{\text{$5$ matches + bonus}\} = \frac{6}{\binom{59}{6}} = 1.33 \times 10^{-7} .\]</span></p>
<p>Other smaller prizes are given for fewer matches.
The corresponding probabilities are:
<span class="math display">\[\begin{align*}
P \{\text{$5$ matches}\} &amp;= \frac{\binom{6}{5} \binom{53}{1}}{\binom{59}{6}} = 7.06 \times 10^{-6}. \\
P \{\text{$4$ matches}\} &amp;= \frac{\binom{6}{4} \binom{53}{2}}{\binom{59}{6}} = 0.000459.\\
P \{\text{$3$ matches}\} &amp;=\frac{\binom{6}{3} \binom{53}{3}}{\binom{59}{6}} = 0.0104.
\end{align*}\]</span></p>
</div>
</div>
</div>
<div id="conditional-probability-and-bayes-theorem" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Conditional probability and Bayes’ Theorem<a href="intro-prob.html#conditional-probability-and-bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How can we use additional information, i.e. things that have already happened, in
the calculation of probabilities? For example, a person may have a certain disease, e.g. diabetes or
HIV/AIDS, whether or not they show any symptoms of it. Suppose a randomly selected person is
found to have the symptom. Given this additional information, what is the probability that they
have the disease? Note that having the symptom does not fully guarantee that the person has the
disease.</p>
<p>Applications of conditional probability occur naturally in actuarial science and medical studies, where conditional probabilities such as “what is the probability that a person will survive for
another 20 years given that they are still alive at the age of 40?” are calculated.
In many real problems, one has to determine the probability of an event A when one already
has some partial knowledge of the outcome of an experiment, i.e. another event B has already
occurred. For this, one needs to find the conditional probability.</p>
<div class="example">
<p><span id="exm:unlabeled-div-10" class="example"><strong>Example 2.10  (Die throw continued) </strong></span>Return to the rolling of a fair die (Example <a href="intro-prob.html#exm:die-throw">2.1</a>). Let
<span class="math display">\[\begin{align*}
A = \{\text{a number greater than $3$}\} = \{4, 5, 6\},
B = \{\text{an even number}\} = \{2, 4, 6\}.
\end{align*}\]</span>
It is clear that <span class="math inline">\(P \{B\} = 3/6 = 1/2\)</span>. This is the unconditional probability of the event <span class="math inline">\(B\)</span>. It is
sometimes called the <em>prior</em> probability of <span class="math inline">\(B\)</span>.</p>
<p>However, suppose that we are told that the event <span class="math inline">\(A\)</span> has already occurred. What is the
probability of <span class="math inline">\(B\)</span> now given that <span class="math inline">\(A\)</span> has already happened?</p>
<p>The sample space of the experiment is <span class="math inline">\(S = \{1, 2, 3, 4, 5, 6\}\)</span>, which contains <span class="math inline">\(n = 6\)</span>
equally likely outcomes.</p>
<p>Given the partial knowledge that event <span class="math inline">\(A\)</span> has occurred, only the <span class="math inline">\(n_A = 3\)</span> outcomes in
<span class="math inline">\(A = \{4, 5, 6\}\)</span> could have occurred. However, only some of the outcomes in <span class="math inline">\(B\)</span> among these
<span class="math inline">\(n_A\)</span> outcomes in <span class="math inline">\(A\)</span> will make event <span class="math inline">\(B\)</span> occur; the number of such outcomes is given by the
number of outcomes
<span class="math inline">\(n_{A\cap B}\)</span> in both <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, i.e., <span class="math inline">\(A \cap B\)</span>, and equal to <span class="math inline">\(2\)</span>.
Hence the probability of <span class="math inline">\(B\)</span>, given the partial
knowledge that event <span class="math inline">\(A\)</span> has occurred, is equal to
<span class="math display">\[\frac{2}{3} = \frac{n_{A \cap B}}{n_A} = \frac{n_{A\cap B} / n}{n_A / n}
= \frac{P\{A \cap B\}}{P\{A\}}\]</span>
Hence we say that <span class="math inline">\(P \{B|A\} = 2/3\)</span>, which is often interpreted as
the <em>posterior</em> probability of <span class="math inline">\(B\)</span> given
<span class="math inline">\(A\)</span>. The additional knowledge that <span class="math inline">\(A\)</span> has already occurred has helped us to
revise the prior probability of <span class="math inline">\(1/2\)</span> to <span class="math inline">\(2/3\)</span>.</p>
</div>
<p>This simple example leads to the following general definition of conditional probability.</p>
<div id="definition-of-conditional-probability" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> Definition of conditional probability<a href="intro-prob.html#definition-of-conditional-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> with <span class="math inline">\(P \{A\} &gt; 0\)</span>, the conditional probability of event
<span class="math inline">\(B\)</span>, given that event <span class="math inline">\(A\)</span> has
occurred, is
<span class="math display">\[P \{B|A\} = \frac{P \{A \cap B\}}{P \{A\}}.\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-11" class="example"><strong>Example 2.11  </strong></span>Of all individuals buying a mobile phone, 60% include a 64 GB hard disk in their
purchase, 40% include a 16 MP camera and 30% include both. If a randomly selected purchase
includes a 16 MP camera, what is the probability that a 64GB hard disk is also included?
The conditional probability is given by
<span class="math display">\[P \{\text{$64$ GB}|\text{$16$ MP}\} =
  \frac{P \{\text{$64$ GB} \cap \text{$16$ MP}\}}{P\{\text{$16$ MP}\}}
  = \frac{0.3}{0.4} = 0.75.\]</span></p>
</div>
</div>
<div id="multiplication-rule-of-conditional-probability" class="section level3 hasAnchor" number="2.5.2">
<h3><span class="header-section-number">2.5.2</span> Multiplication rule of conditional probability<a href="intro-prob.html#multiplication-rule-of-conditional-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>By rearranging the conditional probability definition, we obtain the multiplication rule of conditional probability:
<span class="math display">\[P\{A \cap B\} = P\{A\} P\{B|A\}.\]</span>
Clearly the roles of A and B could be interchanged:
<span class="math display">\[P\{A \cap B\} = P\{B\} P\{A|B\}.\]</span>
Hence the multiplication rule of conditional probability for two events is
<span class="math display">\[P\{A \cap B\} = P\{B\}P\{A|B\} = P\{A\}P\{B|A\}.\]</span></p>
<p>It is straightforward to show by mathematical induction the following multiplication rule of conditional probability for <span class="math inline">\(k(\geq 2)\)</span> events <span class="math inline">\(A_1 , A_2 , \ldots , A_k\)</span>:
<span class="math display">\[P \{A_1 \cap A_2 \cap \ldots \cap A_k\} = P \{A_1\}P \{A_2 |A_1\} P\{A_3 |A_1 \cap A_2\} \ldots P\{A_k |A_1 \cap A_2 \cap \ldots \cap A_{k-1}\}.\]</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-12" class="example"><strong>Example 2.12  (Selecting a committee continued) </strong></span>Return to the committee selection example
(Example <a href="intro-prob.html#exm:committee">2.8</a>),
where there are 4 men (M) and 6 women (W). We want to select a 2-person committee.
Find:</p>
<ol style="list-style-type: lower-roman">
<li>the probability that both are men,</li>
<li>the probability that one is a man and the other is a woman.</li>
</ol>
<p>We have already dealt with this type of urn problem by using the combinatorial method. Here,
the multiplication rule is used instead.
Let <span class="math inline">\(M_i\)</span> be the event that the <span class="math inline">\(i\)</span>th person is a man, and <span class="math inline">\(W_i\)</span> be the event that the <span class="math inline">\(i\)</span>th
person is a woman, <span class="math inline">\(i = 1, 2\)</span>. Then
<span class="math display">\[\text{Prob in (i)} = P \{M_1 \cap M_2 \} = P \{M_1\}P \{M_2 |M_1\} = \frac{4}{10} \times \frac{3}{9},\]</span>
<span class="math display">\[\begin{align*}
\text{Prob in (ii)} &amp;= P \{M_1 \cap W_2 \}+P \{W_1 \cap M_2 \} \\
&amp;= P \{M_1\}P \{W_2 |M_1 \}+P \{W_1\}P \{M_2 |W_1\} \\
&amp;= \frac{4}{10} \times \frac{6}{9} + \frac{6}{10} \times \frac{4}{9}
\end{align*}\]</span></p>
<p>You can find the probability that ‘both are women’ in a similar way.</p>
</div>
</div>
<div id="total-probability-formula" class="section level3 hasAnchor" number="2.5.3">
<h3><span class="header-section-number">2.5.3</span> Total probability formula<a href="intro-prob.html#total-probability-formula" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="example">
<p><span id="exm:unlabeled-div-13" class="example"><strong>Example 2.13  (Phones) </strong></span>Suppose that in our world there are only three phone manufacturing
companies: A Pale, B Sung and C Windows, and their market shares are respectively 30, 40 and 30
percent. Suppose also that respectively 5, 8, and 10 percent of their phones become faulty within
one year. If I buy a phone randomly (ignoring the manufacturer), what is the probability that my
phone will develop a fault within one year? After finding the probability, suppose that my phone
developed a fault in the first year — what is the probability that it was made by A Pale?</p>
<table class="table" style="margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Company
</th>
<th style="text-align:left;">
Market share
</th>
<th style="text-align:left;">
Percent defective
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
A Pale
</td>
<td style="text-align:left;">
30%
</td>
<td style="text-align:left;">
5%
</td>
</tr>
<tr>
<td style="text-align:left;">
B Sung
</td>
<td style="text-align:left;">
40%
</td>
<td style="text-align:left;">
8%
</td>
</tr>
<tr>
<td style="text-align:left;">
C Windows
</td>
<td style="text-align:left;">
30%
</td>
<td style="text-align:left;">
10%
</td>
</tr>
</tbody>
</table>
</div>
<p>To answer this type of question, we derive two of the most useful results in probability theory:
the total probability formula and Bayes’ theorem. First, let us derive the total probability
formula.</p>
<p>Let <span class="math inline">\(B_1, B_2, \ldots , B_k\)</span> be a set of mutually exclusive, i.e.
<span class="math display">\[B_i \cap B_j = \emptyset, \text{for all $1 \leq i \not = j \leq k$.}\]</span>
and exhaustive events, i.e.:
<span class="math display">\[B_1 \cup B_2 \cup \ldots \cup B_k = S.\]</span>
Now any event <span class="math inline">\(A\)</span> can be represented by
<span class="math display">\[A = A \cap S = (A \cap B_1) \cup (A \cap B_2) \cup \ldots \cup (A \cap B_k)\]</span>
where <span class="math inline">\((A \cap B_1), (A \cap B_2), \ldots , (A \cap B_k)\)</span>
are mutually exclusive events. Hence the Axiom A3 of probability gives
<span class="math display">\[\begin{align*}
P \{A\} &amp;= P \{A \cap B_1 \} + P \{A \cap B_2 \} + . . . + P \{A \cap B_k \} \\
&amp;= P \{B_1 \}P \{A|B_1 \} + P \{B_2 \}P \{A|B_2 \} + . . . + P \{B_k \}P \{A|B_k \}.
\end{align*}\]</span>
This last expression is called the <em>total probability formula</em> for <span class="math inline">\(P \{A\}\)</span>.</p>
<div class="example">
<p><span id="exm:unlabeled-div-14" class="example"><strong>Example 2.14  (Phones continued) </strong></span>We can now find the probability of the event, say <span class="math inline">\(A\)</span>, that
a randomly selected phone develops a fault within one year. Let <span class="math inline">\(B_1, B_2, B_3\)</span> be the events that the
phone is manufactured respectively by companies A Pale, B Sung and C Windows. Then we have:
<span class="math display">\[\begin{align*}
P \{A\} &amp;= P \{B_1 \}P \{A|B_1 \} + P \{B_2 \}P \{A|B_2 \} + P \{B_3 \}P \{A|B_3 \} \\
&amp;= 0.30 \times 0.05 + 0.40 \times 0.08 + 0.30 \times 0.10 \\
&amp;= 0.077.
\end{align*}\]</span></p>
<p>Now suppose that my phone has developed a fault within one year. What is the probability that
it was manufactured by A Pale? To answer this we need to introduce Bayes’ Theorem.</p>
</div>
</div>
<div id="bayes-theorem" class="section level3 hasAnchor" number="2.5.4">
<h3><span class="header-section-number">2.5.4</span> Bayes’ theorem<a href="intro-prob.html#bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:unlabeled-div-15" class="theorem"><strong>Theorem 2.1  (Bayes' Theorem) </strong></span>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be events. Then
<span class="math display">\[P \{B |A\} = \frac{P \{B \}P \{A|B \}}{P \{A\}}.\]</span></p>
</div>
<div class="proof">
<p><span id="unlabeled-div-16" class="proof"><em>Proof</em>. </span>From the definition of conditional probability, we have
<span class="math display">\[P \{B |A\} =
  \frac{P \{B \cap A\}}{P \{A\}} =
  \frac{P \{B \}P \{A|B \}}{P \{A\}}.\]</span></p>
</div>
<p>The probability <span class="math inline">\(P \{B |A\}\)</span> is called the posterior probability of
<span class="math inline">\(B\)</span> given <span class="math inline">\(A\)</span> and <span class="math inline">\(P \{B \}\)</span> is called the prior
probability.
Bayes’ theorem is the rule that converts the prior probability into the
posterior probability by using the additional information that some other event,
<span class="math inline">\(A\)</span> above, has already
occurred.</p>
<div class="example">
<p><span id="exm:unlabeled-div-17" class="example"><strong>Example 2.15  (Phones continued) </strong></span>The probability that my faulty phone was manufactured
by A Pale is
<span class="math display">\[P \{B_1 |A\} = \frac{P \{B_1 \}P \{A|B_1 \}}{P \{A\}} =
\frac{0.30 \times 0.05}{0.077}
= 0.1948.\]</span>
Similarly, the probability that the faulty phone was manufactured by B Sung is <span class="math inline">\(0.4156\)</span>, and the
probability that it was manufactured by C Windows is <span class="math inline">\(1-0.1948-0.4156 = 0.3896\)</span>.</p>
</div>
<p>As in this example, we usually need to use the total probability formula
to calculate the denominator <span class="math inline">\(P(A)\)</span> in Bayes’ theorem.</p>
</div>
</div>
<div id="independent-events" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Independent events<a href="intro-prob.html#independent-events" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="introduction-and-definition-of-independence" class="section level3 hasAnchor" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Introduction and definition of independence<a href="intro-prob.html#introduction-and-definition-of-independence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have just seen that the probability of an event may change if we have additional
information. However, in many situations the probabilities may not change. For example, the
results of two coin tosses should not depend on each other.
In this section we will learn about the probabilities of independent events. Much of statistical
theory relies on the concept of independence.</p>
<p>We have seen examples where prior knowledge that an event <span class="math inline">\(A\)</span> has occurred has changed
the probability that event <span class="math inline">\(B\)</span> occurs. There are many situations where this does not happen.
The events
are then said to be independent.
Intuitively, events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if the occurrence of one event does not affect the
probability that the other event occurs.
This is equivalent to saying that
<span class="math inline">\(P \{B|A\} = P \{B\}\)</span>, where <span class="math inline">\(P \{A\} &gt; 0\)</span>, and <span class="math inline">\(P \{A|B\} = P \{A\}\)</span>, where <span class="math inline">\(P \{B\} &gt; 0\)</span>.</p>
<p>These give the following formal definition:
<span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>independent</em> events if <span class="math inline">\(P \{A \cap B\} = P \{A\}P \{B\}.\)</span></p>
<div class="example">
<p><span id="exm:unlabeled-div-18" class="example"><strong>Example 2.16  (Die throw) </strong></span>Throw a fair die. Let <span class="math inline">\(A\)</span> be the event that “the result is even” and <span class="math inline">\(B\)</span> be the event that “the
result is greater than 3”. We want to show that <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not independent.</p>
<p>For this, we have <span class="math inline">\(P \{A \cap B\} = P \{\text{either a $4$ or $6$ thrown}\} = 1/3\)</span>,
but <span class="math inline">\(P \{A\} = 1/2\)</span> and <span class="math inline">\(P \{B\} = 1/2\)</span>, so that
<span class="math inline">\(P \{A\}P \{B\} = 1/4 \not = 1/3 = P \{A \cap B\}\)</span>.
Therefore <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are not independent events.</p>
</div>
<p>Independence is often assumed on physical grounds, although sometimes incorrectly. There are
serious consequences for wrongly assuming independence, e.g. the financial crisis in 2008. However,
when the events are independent then the simpler product formula for joint probability is then used.</p>
<div class="example">
<p><span id="exm:unlabeled-div-19" class="example"><strong>Example 2.17  (Dice throw) </strong></span>Two fair dice when shaken together are assumed to behave independently. Hence
the probability of two sixes is <span class="math inline">\(1/6 \times 1/6 = 1/36\)</span>.</p>
</div>
<div class="example">
<p><span id="exm:unlabeled-div-20" class="example"><strong>Example 2.18  (Assessing risk in legal cases) </strong></span>There have been some disastrous
miscarriages of justice as a result of incorrect assumption of independence. Please read “Incorrect
use of independence — Sally Clark Case” on Blackboard.
<!-- TODO: get/write this document --></p>
</div>
<div class="theorem">
<p><span id="thm:unlabeled-div-21" class="theorem"><strong>Theorem 2.2  (Independence of complementary events) </strong></span>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent, so are <span class="math inline">\(A^\prime\)</span>
and <span class="math inline">\(B^\prime\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-22" class="proof"><em>Proof</em>. </span>Given that
<span class="math inline">\(P \{A \cap B\} = P \{A\}P \{B\}\)</span>, we need to show that
<span class="math inline">\(P \{A^\prime \cap B^\prime \} = P \{A^\prime \}P \{B^\prime \}\)</span>.
We have
<span class="math display">\[\begin{align*}
P \{A^\prime \cap B^\prime \} &amp;= 1 - P \{A \cup B\} \\
&amp;= 1 - [P \{A\} + P \{B\} - P \{A \cap B\}] \\
&amp;= 1 - [P \{A\} + P \{B\} - P \{A\}P \{B\}] \\
&amp;= [1 - P \{A\}] - P \{B\}[1 - P \{A\}] \\
&amp; = [1 - P \{A\}][1 - P \{B\}] \\
&amp;= P \{A^\prime \}P \{B^\prime \}
\end{align*}\]</span></p>
</div>
</div>
<div id="independence-with-three-events" class="section level3 hasAnchor" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Independence with three events<a href="intro-prob.html#independence-with-three-events" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The ideas of conditional probability and independence can be extended to more than two events.</p>
<p>Three events <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are defined to be independent if
<span class="math display" id="eq:pairwise-indep">\[\begin{equation}
P \{A \cap B\} = P \{A\}P \{B\}, \; P \{A \cap C\} = P \{A\}P \{C\}, \; P \{B \cap C\} = P \{B\}P \{C\},
\tag{2.1}
\end{equation}\]</span>
<span class="math display" id="eq:threewise-indep">\[\begin{equation}
P \{A \cap B \cap C\} = P \{A\}P \{B\}P \{C\}.
\tag{2.2}
\end{equation}\]</span></p>
<p>Note that <a href="intro-prob.html#eq:pairwise-indep">(2.1)</a> does NOT imply <a href="intro-prob.html#eq:threewise-indep">(2.2)</a>
as shown by the next example. Hence, to show the
independence of <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span>, it is necessary to show that both
<a href="intro-prob.html#eq:pairwise-indep">(2.1)</a> and <a href="intro-prob.html#eq:threewise-indep">(2.2)</a> hold.</p>
<div class="example">
<p><span id="exm:unlabeled-div-23" class="example"><strong>Example 2.19  </strong></span>A box contains eight tickets, each labelled with a binary number. Two are
labelled with the binary number <span class="math inline">\(111\)</span>, two are labelled with <span class="math inline">\(100\)</span>, two with <span class="math inline">\(010\)</span> and two with <span class="math inline">\(001\)</span>.
An experiment consists of drawing one ticket at random from the box.
Let <span class="math inline">\(A\)</span> be the event “the first digit is 1”, <span class="math inline">\(B\)</span> the event “the second digit is 1”
and <span class="math inline">\(C\)</span> be the event
“the third digit is 1”.
It is clear that <span class="math inline">\(P \{A\} = P \{B\} = P \{C\} = 4/8 = 1/2\)</span> and
<span class="math inline">\(P \{A \cap B\} = P \{A \cap C\} = P \{B \cap C\} = 1/4\)</span>,
so the events are pairwise independent, i.e. <a href="intro-prob.html#eq:pairwise-indep">(2.1)</a> holds.
However
<span class="math inline">\(P \{A \cap B \cap C\} = 2/8 \not = P \{A\}P \{B\}P \{C\} = 1/8\)</span>.
So <a href="intro-prob.html#eq:threewise-indep">(2.2)</a> does not hold and <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> are not
independent.</p>
</div>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distributions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["MATH1063.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
